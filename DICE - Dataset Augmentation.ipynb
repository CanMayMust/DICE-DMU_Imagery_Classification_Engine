{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMU-Net Dataset Augmentation Notebook\n",
    "\n",
    "* **Creator:** Jonathan DEKHTIAR\n",
    "* **Date:** 2017-05-21\n",
    "<br/><br/>\n",
    "* **Contact:** [contact@jonathandekhtiar.eu](mailto:contact@jonathandekhtiar.eu)\n",
    "* **Twitter:** [@born2data](https://twitter.com/born2data)\n",
    "* **LinkedIn:** [JonathanDEKHTIAR](https://fr.linkedin.com/in/jonathandekhtiar)\n",
    "* **Personal Website:** [JonathanDEKHTIAR](http://www.jonathandekhtiar.eu)\n",
    "* **RSS Feed:** [FeedCrunch.io](https://www.feedcrunch.io/@dataradar/)\n",
    "* **Tech. Blog:** [born2data.com](http://www.born2data.com/)\n",
    "* **Github:** [DEKHTIARJonathan](https://github.com/DEKHTIARJonathan)\n",
    "<br/><br/>\n",
    "\n",
    "```\n",
    "*************************************************************************\n",
    "**\n",
    "** 2017 March 13\n",
    "**\n",
    "** In place of a legal notice, here is a blessing:\n",
    "**\n",
    "**    May you do good and not evil.\n",
    "**    May you find forgiveness for yourself and forgive others.\n",
    "**    May you share freely, never taking more than you give.\n",
    "**\n",
    "*************************************************************************\n",
    "```\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In order to maximise the robustness of the re-trained model, each image in the dataset will be loaded and augmented.\n",
    "\n",
    "The augmentation process consists in varying image characteristics such as *brightness, saturation, hue, contrast, gamma, orientation, etc.* These modifications applied to the image are randomly set. \n",
    "\n",
    "This process tends to improve the generalisation power of the model. The number of augmented images generated directly impact the training time and the memory requirements, thus leading to a tradeoff between memory, computing power and the model accuracy.\n",
    "\n",
    "For this study, we have chosen to generate 30 augmented + the original image leading to 31 images per image in the dataset.\n",
    "\n",
    "This notebook will also randomly split the available data into two sets of data: [Training and Validation sets](https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set). This process aims to reduce the [overfit](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html) of the model and thus improving its accuracy on previously unseen data. In this study the selection ratio has been chosen as followed:\n",
    "- *training set:* 60%\n",
    "- *validation set:* 40%.\n",
    "\n",
    "\n",
    "## 1. Load the necessary libraries and initialise global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, string, random\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "################################## GLOBAL NOTEBOOK VARS ##################################\n",
    "\n",
    "NUM_EPOCH               = 1\n",
    "INPUT_DIRECTORY         = \"data\"\n",
    "OUTPUT_DIRECTORY        = \"data_augmented\"\n",
    "\n",
    "TRAINING_DIR_NAME       = \"train\"\n",
    "VALIDATION_DIR_NAME     = \"val\"\n",
    "\n",
    "TRAIN_VAL_SPLIT         = 0.6 # 60% of the images are training data, 40% are validation data\n",
    "IMG_AUGMENTATION_FACTOR = 30 # The number of augmented images generated from the raw image.\n",
    "\n",
    "############################### RANDOM VALUE GENERATION SEED #############################\n",
    "\n",
    "SEED                    = 666\n",
    "\n",
    "######################## Model Dependant Parameters - Inception V3 #######################\n",
    "\n",
    "IMG_HEIGHT              = 299     # This parameter is fixed due to the model used: Inception-V3\n",
    "IMG_WIDTH               = 299     # This parameter is fixed due to the model used: Inception-V3\n",
    "IMG_CHANNELS            = 3       # This parameter is fixed due to the model used: Inception-V3\n",
    "IMG_COLORSPACE          = \"RGB\"   # This parameter is fixed due to the model used: Inception-V3\n",
    "IMG_OUTFORMAT           = \"JPEG\"  # This parameter is fixed due to the model used: Inception-V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Queue and Image Reading Process Definition\n",
    "\n",
    "### 2.1 Define a queue of all the images in \"jpeg\" in the specific data folder\n",
    "\n",
    "Make a queue of file names including all the JPEG images files in the relative image directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_directories = [ name for name in os.listdir(INPUT_DIRECTORY) if os.path.isdir(os.path.join(INPUT_DIRECTORY, name)) ]\n",
    "\n",
    "png_ext_list  = [\"png\"]\n",
    "jpeg_ext_list = [\"jpg\", \"jpeg\"]\n",
    "\n",
    "ext_list = jpeg_ext_list + png_ext_list # = ['jpg', 'jpeg', 'png']\n",
    "\n",
    "all_files = [tf.train.match_filenames_once(INPUT_DIRECTORY + \"/\" + x + \"/*.\"+ext) for x in data_directories for ext in ext_list]\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    tf.concat(all_files,0), # Merge the sub-tensors into one\n",
    "    num_epochs=NUM_EPOCH,\n",
    "    seed=SEED,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Define the image reader\n",
    "\n",
    "Read an entire image file which is required since they're JPEGs, if the images are too large they could be split in advance to smaller files or use the Fixed reader to split up the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_reader = tf.WholeFileReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Read images from the Queue One by One\n",
    "Read a whole file from the queue, the first returned value in the tuple is the filename which we are ignoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_path, image_file = image_reader.read(filename_queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Convert each Image to a Tensor\n",
    "\n",
    "Decode the image file, this will turn it into a Tensor which we can then use in training. It automatically detect whether the image is [\"GIF\", \"PNG\", \"JPEG\"] and which decoder to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_length_tf(t):\n",
    "    return tf.py_func(lambda x: len(x), [t], tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_length = string_length_tf(image_path)\n",
    "file_extension = tf.substr(image_path, path_length - 3, 3)\n",
    "\n",
    "file_cond = tf.equal(file_extension, jpeg_ext_list)\n",
    "file_cond = tf.count_nonzero(file_cond)\n",
    "file_cond = tf.equal(file_cond, 1) ## 1 => JPEG EXTENSION, 0 => PNG EXTENSION\n",
    "        \n",
    "image_tmp      = tf.cond(\n",
    "                    file_cond, \n",
    "                    lambda: tf.image.decode_jpeg(image_file), \n",
    "                    lambda: tf.image.decode_png(image_file)\n",
    "               )\n",
    "\n",
    "image_resized  = tf.image.resize_images(\n",
    "                    image_tmp, \n",
    "                    tf.stack([IMG_HEIGHT, IMG_WIDTH]), \n",
    "                    method=tf.image.ResizeMethod.BICUBIC,\n",
    "                    align_corners=True\n",
    "               )\n",
    "\n",
    "# resize image by bilinear, bicubic and area will change image data type(from uint8 to float32)\n",
    "image_data = tf.cast(image_resized, tf.uint8) # We need to convert it back to unint8 to display it properly\n",
    "\n",
    "image_label    = tf.string_split([image_path] , delimiter=os.path.sep).values[1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Determining whether an image will be used for validation or training at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_train_val     = tf.random_uniform([], 0, 1)\n",
    "\n",
    "is_training_data = tf.less(is_train_val, TRAIN_VAL_SPLIT, name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform Image Augmentation\n",
    "\n",
    "### 4.1 Define an Image Augmentation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_image(image):\n",
    "    \n",
    "    ### GAMMA SHIFTING => It affects primarily the high lights ###\n",
    "    \n",
    "    random_gamma      = tf.random_uniform([], 0.5, 1.1)\n",
    "    image_aug         = image ** random_gamma\n",
    "    \n",
    "    ### BRIGHTNESS SHIFTING ###\n",
    "    \n",
    "    # This gives a centered random  image*(1 +/- delta)\n",
    "    # It does not fit our requirements, we would like a random brightness not centered around \"1\".\n",
    "    #image = tf.image.random_brightness(image, max_delta=0.125) \n",
    "    \n",
    "    random_brightness = tf.random_uniform([], 0.5, 1.2)\n",
    "    image         =  image * random_brightness\n",
    "    \n",
    "    ### OPS SHIFTING ###   \n",
    "    \n",
    "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "    image = tf.image.random_hue(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
    "    \n",
    "    # randomly horizontally flip the image\n",
    "    do_flip = tf.random_uniform([], 0, 1)\n",
    "    image  = tf.cond(do_flip > 0.5, lambda: tf.image.flip_left_right(image), lambda: image)\n",
    "    \n",
    "    # randomly rotate the image\n",
    "    n_rot = tf.random_uniform([], 0, 3, tf.int32) # 0 => No Rotation, 1 => 90° Rot, 2 => 180° Rot, 3 => 270° Rotation\n",
    "    image = tf.image.rot90(image, n_rot)\n",
    "    \n",
    "     # The random_* ops do not necessarily clamp.\n",
    "    image = tf.clip_by_value(image, 0.0, 255.0)\n",
    "    \n",
    "    return tf.cast(image, tf.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Create a Tensor of Images and Populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_arr = tf.stack([\n",
    "    tf.image.encode_jpeg(image_data),\n",
    "])\n",
    "\n",
    "for _ in range(IMG_AUGMENTATION_FACTOR):\n",
    "    img_arr = tf.concat([img_arr, [tf.image.encode_jpeg(augment_image(image_resized))]], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define a function generating random filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id_generator(size=20, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.choice(chars) for _ in range(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define an Initialisation Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op_global = tf.global_variables_initializer()\n",
    "init_op_local = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Launch the dataset generation Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Image: 300\n",
      "Training-Validation Proportion: 57.33%\n",
      "\n",
      "Processing Image: 600\n",
      "Training-Validation Proportion: 56.67%\n",
      "\n",
      "Processing Image: 900\n",
      "Training-Validation Proportion: 58.22%\n",
      "\n",
      "Processing Image: 1200\n",
      "Training-Validation Proportion: 59.42%\n",
      "\n",
      "Processing Image: 1500\n",
      "Training-Validation Proportion: 60.00%\n",
      "\n",
      "Processing Image: 1800\n",
      "Training-Validation Proportion: 59.44%\n",
      "\n",
      "Processing Image: 2100\n",
      "Training-Validation Proportion: 59.10%\n",
      "\n",
      "Processing Image: 2400\n",
      "Training-Validation Proportion: 58.54%\n",
      "\n",
      "Processing Image: 2700\n",
      "Training-Validation Proportion: 58.96%\n",
      "\n",
      "Processing Image: 3000\n",
      "Training-Validation Proportion: 59.43%\n",
      "\n",
      "Processing Image: 3300\n",
      "Training-Validation Proportion: 59.48%\n",
      "\n",
      "Processing Image: 3600\n",
      "Training-Validation Proportion: 59.28%\n",
      "\n",
      "\n",
      "Number of Images Processed: 3670\n",
      "Number of Training Images: 2180\n",
      "Number of Validation Images: 1490\n",
      "Training-Validation Proportion: 59.40%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([init_op_global, init_op_local])\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        i = 0\n",
    "        i_trn = 0 # Counter of training data\n",
    "        i_val = 0 # Counter of validation data\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "            \n",
    "            _trn_bool, _lbl_txt, _img_arr = sess.run([is_training_data, image_label, img_arr])   \n",
    "            \n",
    "            ## Increment ops count\n",
    "            i += 1 \n",
    "\n",
    "            if (_trn_bool):\n",
    "                out_dir = OUTPUT_DIRECTORY + \"/\" + TRAINING_DIR_NAME + \"/\" + _lbl_txt.decode(\"utf-8\")\n",
    "                i_trn += 1\n",
    "                \n",
    "            else:                \n",
    "                out_dir = OUTPUT_DIRECTORY + \"/\" + VALIDATION_DIR_NAME + \"/\" + _lbl_txt.decode(\"utf-8\")\n",
    "                i_val += 1\n",
    "            \n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "                 \n",
    "            for _img in _img_arr:\n",
    "                filename = out_dir + \"/\" + id_generator() + \".jpg\"\n",
    "\n",
    "                with open(filename, \"wb+\") as f:\n",
    "                    f.write(_img)\n",
    "                    f.close()\n",
    "            \n",
    "            if (i % 300 == 0):\n",
    "                print (\"Processing Image:\", i)\n",
    "                print(\"Training-Validation Proportion: %2.2f%%\\n\" % (i_trn/(i_trn+i_val)*100))\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    \n",
    "    finally:        \n",
    "        print(\"\\nNumber of Images Processed:\", i)\n",
    "        print(\"Number of Training Images:\", i_trn)\n",
    "        print(\"Number of Validation Images:\", i_val)\n",
    "        print(\"Training-Validation Proportion: %2.2f%%\" % (i_trn/(i_trn+i_val)*100))\n",
    "        \n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
