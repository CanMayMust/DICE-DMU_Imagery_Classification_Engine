{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DICE - Notebook 3.2 - Model Training and Transfer Learning - Augmented Data\n",
    "\n",
    "<br/>\n",
    "\n",
    "```\n",
    "*************************************************************************\n",
    "**\n",
    "** 2017 Mai 23\n",
    "**\n",
    "** In place of a legal notice, here is a blessing:\n",
    "**\n",
    "**    May you do good and not evil.\n",
    "**    May you find forgiveness for yourself and forgive others.\n",
    "**    May you share freely, never taking more than you give.\n",
    "**\n",
    "*************************************************************************\n",
    "```\n",
    "\n",
    "<table style=\"width:100%; font-size:14px; margin: 20px 0;\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Contact: </b><a href=\"mailto:contact@jonathandekhtiar.eu\" target=\"_blank\">contact@jonathandekhtiar.eu</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Twitter: </b><a href=\"https://twitter.com/born2data\" target=\"_blank\">@born2data</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Tech. Blog: </b><a href=\"http://www.born2data.com/\" target=\"_blank\">born2data.com</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Personal Website: </b><a href=\"http://www.jonathandekhtiar.eu\" target=\"_blank\">jonathandekhtiar.eu</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>RSS Feed: </b><a href=\"https://www.feedcrunch.io/@dataradar/\" target=\"_blank\">FeedCrunch.io</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>LinkedIn: </b><a href=\"https://fr.linkedin.com/in/jonathandekhtiar\" target=\"_blank\">JonathanDEKHTIAR</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook aims to perform the actual transfer learning from the [ImageNet](http://www.image-net.org/) dataset to our custom dataset. For this we will load the model previously trained and retrain the last layers in order to obtain predictions on new classes.\n",
    "\n",
    "A wide variety of models has been trained and made available by the Google Team: https://github.com/tensorflow/models/tree/master/slim\n",
    "\n",
    "We will use in this Notebook, one of the most famous Deep Learning Model: GoogLeNet (aka. Inception-V1) developed by Christian Szegedy and published on ArXiv: https://arxiv.org/abs/1409.4842\n",
    "\n",
    "This notebook will use [Tensorflow-Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) to ease the understanding and reduce the code complexity.\n",
    "\n",
    "Download Inception-V1 Model: http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\n",
    "\n",
    "---\n",
    "\n",
    "As reminder before starting, the data have already been preprocessed (resized, augmented, etc.) in the first Notebook: **[DICE - Notebook 1 - Dataset Augmentation](https://github.com/DEKHTIARJonathan/DICE-DMU_Imagery_Classification_Engine/blob/master/DICE%20-%20Notebook%201%20-%20Dataset%20Augmentation.ipynb)**\n",
    "\n",
    "The preprocessed data all have been saved as **JPEG images** and thus we will only focus on these data.\n",
    "\n",
    "## 1. Notebook Initialisation\n",
    "\n",
    "### 1.1. Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, time, math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2 Initialise global variables and application Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "#State your dataset directory\n",
    "flags.DEFINE_string('dataset_dir', 'data_prepared', 'String: Your dataset directory')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('output_dir', 'output/augmented', 'String: The output directory where model-checkpoints will be saved')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('inception_dir', 'inception_files', 'String: The output directory where model-checkpoints will be saved')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('labels_dir', 'data_prepared', 'String: The output directory where model-checkpoints will be saved')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('tf_record_start_name', 'dmunet_augmented_dataset_', 'String: The output filename to name your TFRecord file')\n",
    "\n",
    "#State the number of epochs to train\n",
    "flags.DEFINE_integer('training_epochs', 10, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#State your batch size => Choose the highest value which doesn't give you a memory error.\n",
    "flags.DEFINE_integer('batch_size', 110, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#Learning rate information and configuration (Up to you to experiment)\n",
    "flags.DEFINE_float('initial_learning_rate', 1e-4, 'Float: The proportion of examples in the dataset to be used for validation')\n",
    "\n",
    "flags.DEFINE_float('learning_rate_decay_factor', 0.8, 'Float: The proportion of examples')\n",
    "\n",
    "flags.DEFINE_integer('num_epochs_before_decay', 1, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "# Choose between \"tf.train.SaverDef.V2\" and \"tf.train.SaverDef.V1\". The V1 version is deprecated since Tensorflow r1.0.0\n",
    "flags.DEFINE_integer('tf_saver', tf.train.SaverDef.V1, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#Set the verbosity to INFO level => highest to lowest logging level: DEBUG > INFO > WARN > ERROR > FATAL  \n",
    "flags.DEFINE_integer('tf_logging_level', tf.logging.INFO, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('checkpoint_basename', 'dmunet_augmented_data.ckpt', 'String: The output filename to name your TFRecord file')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3 Complementary imports from the inception directory set by the flags above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(FLAGS.inception_dir)\n",
    "\n",
    "from preprocessing      import inception_preprocessing\n",
    "from nets.inception_v1  import inception_v1, inception_v1_arg_scope\n",
    "from datasets           import dataset_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Check and Model Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================ Additional Derived Variable ================\n",
    "\n",
    "checkpoint_dir  = os.path.join(FLAGS.inception_dir, \"models\")\n",
    "checkpoint_file = os.path.join(checkpoint_dir, \"inception_v1.ckpt\")\n",
    "labels_file     = os.path.join(FLAGS.labels_dir, \"labels.txt\")\n",
    "\n",
    "image_size      = inception_v1.default_image_size # 224 (width and height in pixels)\n",
    "\n",
    "#Create the file pattern of your TFRecord files so that it could be recognized later on\n",
    "file_pattern    = FLAGS.tf_record_start_name + '%s_*.tfrecord'\n",
    "\n",
    "tf.logging.set_verbosity(FLAGS.tf_logging_level) \n",
    "\n",
    "#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\n",
    "\n",
    "items_to_descriptions = {\n",
    "    'image': 'A 3-channel RGB coloured flower image that is either tulips, sunflowers, roses, dandelion, or daisy.',\n",
    "    'label': 'A label that is as such -- 0:daisy, 1:dandelion, 2:roses, 3:sunflowers, 4:tulips'\n",
    "}\n",
    "\n",
    "# =================== Environment Checking ====================\n",
    "\n",
    "#Create the log directory here. Must be done here otherwise import will activate this unneededly.\n",
    "if not os.path.exists(FLAGS.output_dir):\n",
    "    os.mkdir(FLAGS.output_dir)\n",
    "    \n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "if not os.path.isfile(checkpoint_file):\n",
    "    # We download first the TARGZ archive, if necessary, and then extract it.\n",
    "    \n",
    "    targz = \"inception_v1_2016_08_28.tar.gz\"\n",
    "    url = \"http://download.tensorflow.org/models/\" + targz\n",
    "    \n",
    "    tarfilepath = os.path.join(checkpoint_dir, targz)\n",
    "    \n",
    "    if os.path.isfile(tarfilepath):\n",
    "        import tarfile\n",
    "        tarfile.open(tarfilepath, 'r:gz').extractall(checkpoint_dir)\n",
    "    else:\n",
    "        dataset_utils.download_and_uncompress_tarball(url, checkpoint_dir)\n",
    "        \n",
    "    # Get rid of tarfile source (the checkpoint itself will remain)\n",
    "    os.unlink(tarfilepath)\n",
    "\n",
    "\n",
    "if not os.path.isfile(labels_file):\n",
    "    raise Exception(\"The Label File does not exists\")\n",
    "else:\n",
    "    #State the labels file and read it   \n",
    "    labels = open(labels_file, 'r')\n",
    "    \n",
    "    #Create a dictionary to refer each label to their string name\n",
    "    \n",
    "    labels_to_name = dict()\n",
    "    \n",
    "    for line in labels:\n",
    "        label, string_name = line.split(':')\n",
    "        string_name = string_name[:-1] #Remove newline\n",
    "        labels_to_name[int(label)] = string_name\n",
    "\n",
    "    #State the number of classes to predict\n",
    "    num_classes = len(labels_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============== DATASET LOADING ======================\n",
    "# We now create a function that creates a Dataset class which will give us many TFRecord files \n",
    "#to feed in the examples into a queue in parallel.\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=file_pattern):\n",
    "    '''\n",
    "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
    "    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "    Your file_pattern is very important in locating the files later. \n",
    "\n",
    "    INPUTS:\n",
    "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "    '''\n",
    "\n",
    "    #First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation']: \n",
    "        err = 'The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name)\n",
    "        raise ValueError(err)\n",
    "    \n",
    "    file_pattern_for_counting = file_pattern % (split_name)\n",
    "    \n",
    "    #Count the total number of examples in all of these shard    \n",
    "    tfrecords_to_count = [\n",
    "        os.path.join(dataset_dir, file) \n",
    "        for file in os.listdir(dataset_dir) \n",
    "        if file.startswith(file_pattern_for_counting[:-10]) # We remove the 10 last chars: *.tfrecord   \n",
    "    ]\n",
    "    \n",
    "    num_samples = 0\n",
    "    \n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "            num_samples += 1\n",
    "\n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "      'image/class/label': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image(),\n",
    "        'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "    #Create the labels_to_name file\n",
    "    labels_to_name_dict = labels_to_name\n",
    "    \n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = os.path.join(dataset_dir, file_pattern_for_counting)\n",
    "\n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = file_pattern_path,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        num_readers = 4,\n",
    "        num_samples = num_samples,\n",
    "        num_classes = num_classes,\n",
    "        labels_to_name = labels_to_name_dict,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=True):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "\n",
    "    '''\n",
    "    #First create the data_provider object\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        common_queue_capacity = 24 + 3 * batch_size,\n",
    "        common_queue_min = 24)\n",
    "\n",
    "    #Obtain the raw image using the get method\n",
    "    raw_image, label = data_provider.get(['image', 'label'])\n",
    "\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\n",
    "\n",
    "    #As for the raw images, we just do a simple reshape to batch it up\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    images, raw_images, labels = tf.train.batch(\n",
    "        [image, raw_image, label],\n",
    "        batch_size = batch_size,\n",
    "        num_threads = 4,\n",
    "        capacity = 4 * batch_size,\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return images, raw_images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset and data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = get_split('train', FLAGS.dataset_dir, file_pattern=file_pattern)\n",
    "images, _, labels = load_batch(dataset, batch_size=FLAGS.batch_size)\n",
    "\n",
    "num_steps_per_epoch = math.ceil(dataset.num_samples / FLAGS.batch_size)\n",
    "\n",
    "#Know the number steps to take before decaying the learning rate and batches per epoch\n",
    "decay_steps = FLAGS.num_epochs_before_decay * num_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the model inference\n",
    "with slim.arg_scope(inception_v1_arg_scope()):\n",
    "    logits, end_points = inception_v1(images, num_classes = dataset.num_classes, is_training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the scopes that you want to exclude for restoration\n",
    "exclude              = [\"InceptionV1/Logits\", \"InceptionV1/AuxLogits\"]\n",
    "variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n",
    "variables_to_save    = slim.get_variables_to_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)\n",
    "one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n",
    "total_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the global step for monitoring the learning_rate and training.\n",
    "global_step = get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define your exponentially decaying learning rate\n",
    "lr = tf.train.exponential_decay(\n",
    "    learning_rate = FLAGS.initial_learning_rate,\n",
    "    global_step = global_step,\n",
    "    decay_steps = decay_steps,\n",
    "    decay_rate = FLAGS.learning_rate_decay_factor,\n",
    "    staircase = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we can define the optimizer that takes on the learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the train_op.\n",
    "train_op = slim.learning.create_train_op(total_loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
    "predictions                = tf.argmax(end_points['Predictions'], 1)\n",
    "probabilities              = end_points['Predictions']\n",
    "accuracy, accuracy_update  = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "metrics_op                 = tf.group(accuracy_update, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "my_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n",
    "def train_step(sess, train_op, global_step):\n",
    "    '''\n",
    "    Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n",
    "    '''\n",
    "    #Check the time for each sess run\n",
    "    start_time = time.time()\n",
    "    total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "    #Run the logging to print some results\n",
    "    logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n",
    "\n",
    "    return total_loss, global_step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we create a saver function that actually restores the variables from a checkpoint file in a sess\n",
    "restore_saver = tf.train.Saver(\n",
    "    var_list      = variables_to_restore,\n",
    "    write_version = FLAGS.tf_saver\n",
    ")\n",
    "\n",
    "def restore_fn(sess):\n",
    "    return restore_saver.restore(sess, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define your supervisor for running a managed session. \n",
    "#Do not run the summary_op automatically or else it will consume too much memory\n",
    "\n",
    "saving_saver = tf.train.Saver(\n",
    "    var_list      = variables_to_save,\n",
    "    write_version = FLAGS.tf_saver, \n",
    "    max_to_keep   = FLAGS.training_epochs\n",
    ")\n",
    "\n",
    "sv = tf.train.Supervisor(\n",
    "    logdir                = FLAGS.output_dir, \n",
    "    summary_op            = None, \n",
    "    init_fn               = restore_fn,\n",
    "    checkpoint_basename   = FLAGS.checkpoint_basename,\n",
    "    save_model_secs       = None, # Prevent Automatic Model saving\n",
    "    saver                 = saving_saver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from inception_files\\models\\inception_v1.ckpt\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "\n",
      "###################################\n",
      "\n",
      "Number of Epochs: 10\n",
      "Number of Steps per Epoch: 621\n",
      "Summary Recorded Every 62 Steps\n",
      "\n",
      "total steps: 6210\n",
      "\n",
      "###################################\n",
      "\n",
      "INFO:tensorflow:Epoch 1/10\n",
      "INFO:tensorflow:Current Learning Rate: 0.0001\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.0\n",
      "INFO:tensorflow:global step 1: loss: 1.9852 (2.61 sec/step)\n",
      "INFO:tensorflow:global step 2: loss: 1.7566 (1.17 sec/step)\n",
      "INFO:tensorflow:global step 3: loss: 1.6449 (1.18 sec/step)\n",
      "INFO:tensorflow:global step 4: loss: 1.6816 (1.19 sec/step)\n",
      "INFO:tensorflow:global step 5: loss: 1.5104 (1.26 sec/step)\n",
      "INFO:tensorflow:global step 6: loss: 1.4108 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 7: loss: 1.4286 (1.36 sec/step)\n",
      "INFO:tensorflow:global step 8: loss: 1.4229 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 9: loss: 1.3287 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 10: loss: 1.2043 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 11: loss: 1.1910 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 12: loss: 1.0845 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 13: loss: 1.1129 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 14: loss: 0.9924 (1.36 sec/step)\n",
      "INFO:tensorflow:global step 15: loss: 1.0898 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 16: loss: 1.0271 (1.38 sec/step)\n",
      "INFO:tensorflow:global step 17: loss: 1.0136 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 18: loss: 0.9908 (1.36 sec/step)\n",
      "INFO:tensorflow:global step 19: loss: 1.0378 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 20: loss: 0.9264 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 21: loss: 1.0818 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 22: loss: 0.9009 (1.37 sec/step)\n",
      "INFO:tensorflow:global step 23: loss: 0.8890 (1.38 sec/step)\n",
      "INFO:tensorflow:global step 24: loss: 0.8245 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 25: loss: 0.8210 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 26: loss: 0.8290 (1.40 sec/step)\n",
      "INFO:tensorflow:global step 27: loss: 0.7990 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 28: loss: 0.9651 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 29: loss: 0.9300 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 30: loss: 0.8748 (1.38 sec/step)\n",
      "INFO:tensorflow:global step 31: loss: 0.8093 (1.36 sec/step)\n",
      "INFO:tensorflow:global step 32: loss: 0.7571 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 33: loss: 0.8149 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 34: loss: 0.8167 (1.38 sec/step)\n",
      "INFO:tensorflow:global step 35: loss: 0.7700 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 36: loss: 0.8324 (1.37 sec/step)\n",
      "INFO:tensorflow:global step 37: loss: 0.7314 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 38: loss: 0.7620 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 39: loss: 0.7495 (1.36 sec/step)\n",
      "INFO:tensorflow:global step 40: loss: 0.8100 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 41: loss: 0.7992 (1.37 sec/step)\n",
      "INFO:tensorflow:global step 42: loss: 0.8132 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 43: loss: 0.7694 (1.36 sec/step)\n",
      "INFO:tensorflow:global step 44: loss: 0.7973 (1.37 sec/step)\n",
      "INFO:tensorflow:global step 45: loss: 0.7424 (1.37 sec/step)\n",
      "INFO:tensorflow:global step 46: loss: 0.6012 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 47: loss: 0.7240 (1.38 sec/step)\n",
      "INFO:tensorflow:global step 48: loss: 0.6353 (1.36 sec/step)\n",
      "INFO:tensorflow:global step 49: loss: 0.8177 (1.38 sec/step)\n",
      "INFO:tensorflow:global step 50: loss: 0.6487 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 51: loss: 0.6518 (1.40 sec/step)\n",
      "INFO:tensorflow:global step 52: loss: 0.6594 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 53: loss: 0.6480 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 54: loss: 0.6276 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 55: loss: 0.6211 (1.40 sec/step)\n",
      "INFO:tensorflow:global step 56: loss: 0.7308 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 57: loss: 0.7521 (1.40 sec/step)\n",
      "INFO:tensorflow:global step 58: loss: 0.6546 (1.40 sec/step)\n",
      "INFO:tensorflow:global step 59: loss: 0.6641 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 60: loss: 0.6836 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 61: loss: 0.7519 (1.41 sec/step)\n",
      "INFO:tensorflow:global step 62: loss: 0.7841 (1.41 sec/step)\n",
      "INFO:tensorflow:global step 63: loss: 0.5999 (1.45 sec/step)\n",
      "INFO:tensorflow:global step 64: loss: 0.6595 (1.41 sec/step)\n",
      "INFO:tensorflow:global step 65: loss: 0.6374 (1.41 sec/step)\n",
      "INFO:tensorflow:global step 66: loss: 0.6097 (1.44 sec/step)\n",
      "INFO:tensorflow:global step 67: loss: 0.5700 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 68: loss: 0.6536 (1.41 sec/step)\n",
      "INFO:tensorflow:global step 69: loss: 0.6098 (1.43 sec/step)\n",
      "INFO:tensorflow:global step 70: loss: 0.5828 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 71: loss: 0.6283 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 72: loss: 0.7002 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 73: loss: 0.6838 (1.44 sec/step)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-3d107fc34491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#If not, simply run the training step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m#We log the final training loss and accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-9d7d2ed7179b>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(sess, train_op, global_step)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#Check the time for each sess run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtime_elapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Run the managed session\n",
    "with sv.managed_session() as sess:   \n",
    "    \n",
    "    print(\"\\n###################################\\n\")\n",
    "    \n",
    "    print(\"Number of Epochs: %d\" % FLAGS.training_epochs)\n",
    "    print(\"Number of Steps per Epoch: %d\" % num_steps_per_epoch)\n",
    "    print(\"Summary Recorded Every %d Steps\\n\" % round(num_steps_per_epoch/10))\n",
    "    \n",
    "    print(\"total steps: %d\" % (num_steps_per_epoch * FLAGS.training_epochs))\n",
    "    \n",
    "    print(\"\\n###################################\\n\")\n",
    "    \n",
    "    for step in range(num_steps_per_epoch * FLAGS.training_epochs):\n",
    "        \n",
    "        #At the start of every epoch, show the vital information:\n",
    "        if step % num_steps_per_epoch == 0:\n",
    "            \n",
    "            learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n",
    "            \n",
    "            logging.info('Epoch %d/%d', step/num_steps_per_epoch + 1, FLAGS.training_epochs)\n",
    "            logging.info('Current Learning Rate: %s', learning_rate_value)\n",
    "            logging.info('Current Streaming Accuracy: %s', accuracy_value)\n",
    "            \n",
    "            # We save the model after each epoch\n",
    "            if step != 0:\n",
    "                sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n",
    "\n",
    "        #Log the summaries every 1-10th of epoch.\n",
    "        if (step % num_steps_per_epoch % round(num_steps_per_epoch/10)) == 0 :\n",
    "            loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "            summaries = sess.run(my_summary_op)\n",
    "            sv.summary_computed(sess, summaries)\n",
    "\n",
    "        #If not, simply run the training step\n",
    "        else:\n",
    "            loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "\n",
    "    #We log the final training loss and accuracy\n",
    "    logging.info('Final Loss: %s', loss)\n",
    "    logging.info('Final Accuracy: %s', sess.run(accuracy))\n",
    "\n",
    "    #Once all the training has been done, save the log files and checkpoint model\n",
    "    logging.info('Finished training! Saving model to disk now.')\n",
    "    \n",
    "    sv.saver.save(sess, sv.save_path, global_step = sv.global_step)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
