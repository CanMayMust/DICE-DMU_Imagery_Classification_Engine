{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMU-Net Dataset Generation Notebook\n",
    "\n",
    "* **Creator:** Jonathan DEKHTIAR\n",
    "* **Date:** 2017-05-21\n",
    "<br/><br/>\n",
    "* **Contact:** [contact@jonathandekhtiar.eu](mailto:contact@jonathandekhtiar.eu)\n",
    "* **Twitter:** [@born2data](https://twitter.com/born2data)\n",
    "* **LinkedIn:** [JonathanDEKHTIAR](https://fr.linkedin.com/in/jonathandekhtiar)\n",
    "* **Personal Website:** [JonathanDEKHTIAR](http://www.jonathandekhtiar.eu)\n",
    "* **RSS Feed:** [FeedCrunch.io](https://www.feedcrunch.io/@dataradar/)\n",
    "* **Tech. Blog:** [born2data.com](http://www.born2data.com/)\n",
    "* **Github:** [DEKHTIARJonathan](https://github.com/DEKHTIARJonathan)\n",
    "<br/><br/>\n",
    "\n",
    "```\n",
    "*************************************************************************\n",
    "**\n",
    "** 2017 March 13\n",
    "**\n",
    "** In place of a legal notice, here is a blessing:\n",
    "**\n",
    "**    May you do good and not evil.\n",
    "**    May you find forgiveness for yourself and forgive others.\n",
    "**    May you share freely, never taking more than you give.\n",
    "**\n",
    "*************************************************************************\n",
    "```\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook generates a [TFRecords](https://www.tensorflow.org/api_guides/python/python_io#TFRecords_Format_Details) file from the images contained in the \"./data/\" folder.\n",
    "\n",
    "This will be used later to retrain an CNN model: [Inception-V3](http://arxiv.org/abs/1512.00567) model developed by Szegedy et al. The model has been Pre-Trained with the [ImageNet](http://www.image-net.org/) dataset allowing a much more accurate result due to the large number of data avaiable in this dataset. We call this a \"Transfer Learning\".\n",
    "\n",
    "## 1. Load the necessary libraries and initialise global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "####################### GLOBAL Notebook\n",
    "\n",
    "NUM_EPOCH          = 1\n",
    "INPUT_DIRECTORY    = \"data\"\n",
    "TFRECORD_FILENAME  = \"output/dmu_net.tfrecord\"\n",
    "SEED               = 666 # A specific number for reproducability or None for a random value\n",
    "\n",
    "# Model Dependant Parameters - Inception V3\n",
    "\n",
    "IMG_HEIGHT         = 299     # This parameter is fixed due to the model used: Inception-V3\n",
    "IMG_WIDTH          = 299     # This parameter is fixed due to the model used: Inception-V3\n",
    "IMG_CHANNELS       = 3       # This parameter is fixed due to the model used: Inception-V3\n",
    "IMG_COLORSPACE     = \"RGB\"   # This parameter is fixed due to the model used: Inception-V3\n",
    "IMG_FORMAT         = \"JPEG\"  # This parameter is fixed due to the model used: Inception-V3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. File Queue and Image Reading Process Definition\n",
    "\n",
    "### 2.1 Define a queue of all the images in \"jpeg\" in the specific data folder\n",
    "\n",
    "Make a queue of file names including all the JPEG images files in the relative image directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_directories = [ name for name in os.listdir(INPUT_DIRECTORY) if os.path.isdir(os.path.join(INPUT_DIRECTORY, name)) ]\n",
    "\n",
    "all_files = [tf.train.match_filenames_once(INPUT_DIRECTORY + \"/\" + x + \"/*.jpg\") for x in data_directories]\n",
    "\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    tf.concat(all_files,0), # Merge the sub-tensors into one\n",
    "    num_epochs=NUM_EPOCH,\n",
    "    seed=SEED,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Define the image reader\n",
    "\n",
    "Read an entire image file which is required since they're JPEGs, if the images are too large they could be split in advance to smaller files or use the Fixed reader to split up the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_reader = tf.WholeFileReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Read images from the Queue One by One\n",
    "Read a whole file from the queue, the first returned value in the tuple is the filename which we are ignoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_path, image_file = image_reader.read(filename_queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Convert each Image to a Tensor\n",
    "\n",
    "Decode the image file, this will turn it into a Tensor which we can then use in training. It automatically detect whether the image is [\"GIF\", \"PNG\", \"JPEG\"] and which decoder to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_length_tf(t):\n",
    "    return tf.py_func(lambda x: len(x), [t], tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_length = string_length_tf(image_path)\n",
    "file_extension = tf.substr(image_path, path_length - 3, 3)\n",
    "\n",
    "file_cond = tf.equal(file_extension, 'jpg')\n",
    "        \n",
    "image_tmp      = tf.cond(\n",
    "                    file_cond, \n",
    "                    lambda: tf.image.decode_jpeg(image_file), \n",
    "                    lambda: tf.image.decode_png(image_file)\n",
    "               )\n",
    "\n",
    "image_resized  = tf.image.resize_images(\n",
    "                    image_tmp, \n",
    "                    tf.stack([IMG_HEIGHT, IMG_WIDTH]), \n",
    "                    method=tf.image.ResizeMethod.BICUBIC,\n",
    "                    align_corners=True\n",
    "               )\n",
    "\n",
    "# resize image by bilinear, bicubic and area will change image data type(from uint8 to float32)\n",
    "image_data = tf.cast(image_resized, tf.uint8) # We need to convert it back to unint8 to display it properly\n",
    "\n",
    "image_shape    = tf.shape(image_data)\n",
    "image_height   = image_shape[0]\n",
    "image_width    = image_shape[1]\n",
    "image_channels = image_shape[2]\n",
    "\n",
    "image_label    = tf.string_split([image_path] , delimiter=os.path.sep).values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform Image Augmentation\n",
    "\n",
    "### 3.1 Define an Image Augmentation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image):\n",
    "    # randomly shift gamma\n",
    "    random_gamma      = tf.random_uniform([], 0.8, 1.2)\n",
    "    image_aug         = image ** random_gamma\n",
    "\n",
    "    # randomly shift brightness\n",
    "    random_brightness = tf.random_uniform([], 0.5, 1.2)\n",
    "    image_aug         =  image_aug * random_brightness\n",
    "    \n",
    "    '''\n",
    "    # randomly shift color\n",
    "    random_colors     = tf.random_uniform([3], 0.8, 1.2)\n",
    "    \n",
    "    white             = tf.ones([tf.shape(image)[0], tf.shape(image)[1]])    \n",
    "    color_image       = tf.stack([white * random_colors[i] for i in range(3)], axis=2)\n",
    "    \n",
    "    image_aug         *= color_image\n",
    "    '''\n",
    "    \n",
    "    # saturate\n",
    "    #image_aug         = tf.clip_by_value(image_aug,  0, 255)\n",
    "    \n",
    "    brightest_val = tf.reduce_max(image_aug)\n",
    "    \n",
    "    #image_aug = \n",
    "    image_aug = tf.cond(\n",
    "        brightest_val > 255, \n",
    "        lambda: image_aug * (255.0 / brightest_val), \n",
    "        lambda: image_aug\n",
    "    )\n",
    "    \n",
    "    # randomly horizontally flip the image\n",
    "    do_flip = tf.random_uniform([], 0, 1)\n",
    "    image_aug  = tf.cond(do_flip > 0.5, lambda: tf.image.flip_left_right(image_aug), lambda: image_aug)\n",
    "    \n",
    "    # randomly rotate the image\n",
    "    n_rot = tf.random_uniform([], 0, 3, tf.int32) # 0 => No Rotation, 1 => 90° Rot, 2 => 180° Rot, 3 => 270° Rotation\n",
    "    image_aug = tf.image.rot90(image_aug, n_rot)\n",
    "    \n",
    "    #Convert to UINT-8 and Return\n",
    "    return tf.cast(image_aug, tf.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Create a Tensor of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_arr = tf.stack([\n",
    "    image_data,\n",
    "])\n",
    "\n",
    "for _ in range(30):\n",
    "    img_arr = tf.concat([img_arr, [augment_image(image_resized)]], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the TFRecord Saving Process\n",
    "\n",
    "### 3.1. Defining the tf.train.Features functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Defining the writer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writer_to_tfrecord(height, width, colorspace, channels, label, text, image_format, filename, image_buffer):\n",
    "    \n",
    "    # write label, shape, and image content to the TFRecord file\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={    \n",
    "        'image/height'      : _int64_feature(height),\n",
    "        'image/width'       : _int64_feature(width),\n",
    "        'image/colorspace'  : _bytes_feature(tf.compat.as_bytes(colorspace)),\n",
    "        'image/channels'    : _int64_feature(channels),\n",
    "        'image/class/label' : _int64_feature(label),\n",
    "        'image/class/text'  : _bytes_feature(tf.compat.as_bytes(text)),\n",
    "        'image/format'      : _bytes_feature(tf.compat.as_bytes(image_format)),\n",
    "        'image/filename'    : _bytes_feature(tf.compat.as_bytes(os.path.basename(filename))),\n",
    "        'image/encoded'     : _bytes_feature(tf.compat.as_bytes(image_buffer.tobytes()))\n",
    "    }))\n",
    "    \n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define an Initialisation Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op_global = tf.global_variables_initializer()\n",
    "init_op_local = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch the dataset generation Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Image: 300\n",
      "Processing Image: 600\n",
      "Processing Image: 900\n",
      "Processing Image: 1200\n",
      "Processing Image: 1500\n",
      "Processing Image: 1800\n",
      "Processing Image: 2100\n",
      "Processing Image: 2400\n",
      "Processing Image: 2700\n",
      "Processing Image: 3000\n",
      "Processing Image: 3300\n",
      "Processing Image: 3600\n",
      "\n",
      "Number of Images Processed: 3671\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([init_op_global, init_op_local])\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(TFRECORD_FILENAME)\n",
    "    \n",
    "    try:\n",
    "        i = 1\n",
    "        while not coord.should_stop():\n",
    "            \n",
    "            _h, _w, _chn, _lbl_txt, _img_pth, _img_arr = sess.run([\n",
    "                image_height, \n",
    "                image_width, \n",
    "                image_channels, \n",
    "                image_label, \n",
    "                image_path,\n",
    "                img_arr\n",
    "            ])\n",
    "            \n",
    "            _lbl_idx = data_directories.index(str(_lbl_txt,'utf-8'))\n",
    "            \n",
    "            '''\n",
    "            if i <= 1:\n",
    "                print(\"height:\", _h)\n",
    "                print(\"width:\", _w)\n",
    "                print(\"channels:\", _chn)\n",
    "                print(\"label:\", _lbl_txt)\n",
    "                \n",
    "                for img in _img_arr:\n",
    "                    plt.imshow(img)\n",
    "                    plt.show()\n",
    "\n",
    "                print()\n",
    "            '''\n",
    "            \n",
    "            for _img in _img_arr:\n",
    "                writer_to_tfrecord(\n",
    "                    height        = _h, \n",
    "                    width         = _w, \n",
    "                    colorspace    = IMG_COLORSPACE, \n",
    "                    channels      = IMG_CHANNELS, \n",
    "                    label         = _lbl_idx, \n",
    "                    text          = _lbl_txt, \n",
    "                    image_format  = IMG_FORMAT, \n",
    "                    filename      = _img_pth, \n",
    "                    image_buffer  = _img\n",
    "                )\n",
    "            \n",
    "            if (i % 300 == 0):\n",
    "                print (\"Processing Image:\", i)\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    \n",
    "    finally:        \n",
    "        print(\"\\nNumber of Images Processed:\", i)\n",
    "        \n",
    "        writer.close()\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
