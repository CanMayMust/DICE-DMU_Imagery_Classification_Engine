{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICE - Notebook 2.2 - Dataset Preparation - Augmented Data\n",
    "\n",
    "<br/>\n",
    "\n",
    "```\n",
    "*************************************************************************\n",
    "**\n",
    "** 2017 Mai 23\n",
    "**\n",
    "** In place of a legal notice, here is a blessing:\n",
    "**\n",
    "**    May you do good and not evil.\n",
    "**    May you find forgiveness for yourself and forgive others.\n",
    "**    May you share freely, never taking more than you give.\n",
    "**\n",
    "*************************************************************************\n",
    "```\n",
    "\n",
    "<table style=\"width:100%; font-size:14px; margin: 20px 0;\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Contact: </b><a href=\"mailto:contact@jonathandekhtiar.eu\" target=\"_blank\">contact@jonathandekhtiar.eu</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Twitter: </b><a href=\"https://twitter.com/born2data\" target=\"_blank\">@born2data</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Tech. Blog: </b><a href=\"http://www.born2data.com/\" target=\"_blank\">born2data.com</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Personal Website: </b><a href=\"http://www.jonathandekhtiar.eu\" target=\"_blank\">jonathandekhtiar.eu</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>RSS Feed: </b><a href=\"https://www.feedcrunch.io/@dataradar/\" target=\"_blank\">FeedCrunch.io</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>LinkedIn: </b><a href=\"https://fr.linkedin.com/in/jonathandekhtiar\" target=\"_blank\">JonathanDEKHTIAR</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook aims to preprocess and prepare the dataset for later used during the training phase. \n",
    "\n",
    "There exists many methods to feed data into a Deep Learning with [Tensorflow](https://www.tensorflow.org/), the Python Library we have chosen to use for this study:\n",
    "\n",
    "1. **From Disk**: Data can be inputed into a model with the **feed_dict** argument when running a *training operation*. It would  definitely be possible, however this process can be slow if there are a lot of data to read simultaneously and could be too large to be held in the GPU Memory.\n",
    "<br><br>\n",
    "2. **From a CSV File**: This [type of file](https://en.wikipedia.org/wiki/Comma-separated_values) is not revelant when dealing with images.\n",
    "<br><br>\n",
    "3. **From a preprocessed binary file**: Tensorflow is able to save and recover data in a binary format called [TFRecords](https://www.tensorflow.org/api_guides/python/python_io#TFRecords_Format_Details). The data can be preprocessed beforehand and only the necessary data can be saved and read in real time during the training. This approach is the fatest and most memory-efficient when dealing with images.\n",
    "\n",
    "This notebook will focus on generating the necessary **TFRecord** files. Generating **TFRecords** is less intuitive than \n",
    "[HDF5](https://en.wikipedia.org/wiki/Hierarchical_Data_Format), used in other Deep Learning libraries such as [Keras](https://keras.io/). Using **TFRecords** will give you access to natively available tools, such as *Queue Runners*, *Coordinators*, *Supervisors*, *etc.*, to design [data pipelines](https://www.tensorflow.org/programmers_guide/reading_data) and process the images in a batch fashion.\n",
    "\n",
    "This notebook will use [Tensorflow-Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) to ease the understanding and reduce the code complexity.\n",
    "\n",
    "As we aim to to later re-train a CNN Model\n",
    "\n",
    "This will be used later to retrain an CNN model: [Inception-V4](https://arxiv.org/abs/1602.07261) model developed by Szegedy et al. The model has been Pre-Trained with the [ImageNet](http://www.image-net.org/) dataset allowing a much more accurate result due to the large number of data avaiable in this dataset. We call this kind of process: \"*Transfer Learning*\".\n",
    "\n",
    "\n",
    "This notebook will also randomly split the available data into two sets of data: [Training and Validation sets](https://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set). This process aims to reduce the [overfit](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html) of the model and thus improving its accuracy on previously unseen data. \n",
    "\n",
    "In this study the selection ratio has been chosen as followed:\n",
    "- *training set:* 60%\n",
    "- *validation set:* 40%.\n",
    "\n",
    "---\n",
    "\n",
    "As reminder before starting, the data have already been preprocessed (resized, augmented, etc.) in the first Notebook: **[DICE - Notebook 1 - Dataset Augmentation](https://github.com/DEKHTIARJonathan/DICE-DMU_Imagery_Classification_Engine/blob/master/DICE%20-%20Notebook%201%20-%20Dataset%20Augmentation.ipynb)**\n",
    "\n",
    "The preprocessed data all have been saved as **JPEG images** and thus we will only focus on these data.\n",
    "\n",
    "## 1. Notebook Initialisation\n",
    "\n",
    "### 1.1. Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, random\n",
    "import tensorflow as tf\n",
    "from dataset_utils import _dataset_exists, _get_filenames_and_classes, write_label_file, _convert_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2 Initialise global variables and application Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "#State your dataset directory\n",
    "flags.DEFINE_string('dataset_dir', 'data_augmented/', 'String: Your dataset directory')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('output_dir', 'data_prepared/', 'String: The output filename to name your TFRecord file')\n",
    "\n",
    "# Proportion of dataset to be used for evaluation: 0.4 => 60% Training & 40% Validation\n",
    "flags.DEFINE_float('validation_size', 0.4, 'Float: The proportion of examples in the dataset to be used for validation')\n",
    "\n",
    "# The number of shards to split the dataset into.\n",
    "# Try to select the number of shards such that roughly 1024 images reside in each shard.\n",
    "flags.DEFINE_integer('num_shards_training', 67, 'Int: Number of shards to split the TFRecord files into')\n",
    "flags.DEFINE_integer('num_shards_validation', 44, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "# Seed for repeatability.\n",
    "flags.DEFINE_integer('random_seed', 666, 'Int: Random seed to use for repeatability.')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('tfrecord_filename', 'dmunet_augmented_dataset', 'String: The output filename to name your TFRecord file')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Create the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(FLAGS.output_dir):\n",
    "    os.makedirs(FLAGS.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "photo_filenames, class_names = _get_filenames_and_classes(FLAGS.dataset_dir)  \n",
    "class_names_to_ids = dict(zip(class_names, range(len(class_names))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performing the train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the number of validation examples we need\n",
    "num_validation = int(FLAGS.validation_size * len(photo_filenames))\n",
    "\n",
    "# Divide the training datasets into train and test:\n",
    "random.seed(FLAGS.random_seed)\n",
    "random.shuffle(photo_filenames)\n",
    "training_filenames = photo_filenames[num_validation:]\n",
    "validation_filenames = photo_filenames[:num_validation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Converting the datasets into TFRecords\n",
    "\n",
    "### 4.1. Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00001-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 1000/68262 - shard 1\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00002-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 2000/68262 - shard 2\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00003-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 3000/68262 - shard 3\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00004-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 4000/68262 - shard 4\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00005-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 5000/68262 - shard 5\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00006-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 6000/68262 - shard 6\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00007-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 7000/68262 - shard 7\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00008-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 8000/68262 - shard 8\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00009-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 9000/68262 - shard 9\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00010-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 10000/68262 - shard 10\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00011-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 11000/68262 - shard 11\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00012-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 12000/68262 - shard 12\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00013-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 13000/68262 - shard 13\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00014-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 14000/68262 - shard 14\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00015-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 15000/68262 - shard 15\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00016-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 16000/68262 - shard 16\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00017-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 17000/68262 - shard 17\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00018-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 18000/68262 - shard 18\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00019-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 19000/68262 - shard 19\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00020-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 20000/68262 - shard 20\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00021-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 21000/68262 - shard 21\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00022-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 22000/68262 - shard 22\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00023-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 23000/68262 - shard 23\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00024-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 24000/68262 - shard 24\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00025-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 25000/68262 - shard 25\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00026-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 26000/68262 - shard 26\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00027-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 27000/68262 - shard 27\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00028-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 28000/68262 - shard 28\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00029-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 29000/68262 - shard 29\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00030-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 30000/68262 - shard 30\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00031-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 31000/68262 - shard 31\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00032-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 32000/68262 - shard 32\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00033-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 33000/68262 - shard 33\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00034-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 34000/68262 - shard 34\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00035-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 35000/68262 - shard 35\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00036-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 36000/68262 - shard 36\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00037-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 37000/68262 - shard 37\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00038-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 38000/68262 - shard 38\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00039-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 39000/68262 - shard 39\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00040-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 40000/68262 - shard 40\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00041-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 41000/68262 - shard 41\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00042-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 42000/68262 - shard 42\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00043-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 43000/68262 - shard 43\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00044-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 44000/68262 - shard 44\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00045-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 45000/68262 - shard 45\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00046-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 46000/68262 - shard 46\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00047-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting image 47000/68262 - shard 47\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00048-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 48000/68262 - shard 48\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00049-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 49000/68262 - shard 49\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00050-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 50000/68262 - shard 50\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00051-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 51000/68262 - shard 51\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00052-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 52000/68262 - shard 52\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00053-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 53000/68262 - shard 53\n",
      "Converting image 54000/68262 - shard 53\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00054-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 55000/68262 - shard 54\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00055-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 56000/68262 - shard 55\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00056-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 57000/68262 - shard 56\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00057-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 58000/68262 - shard 57\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00058-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 59000/68262 - shard 58\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00059-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 60000/68262 - shard 59\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00060-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 61000/68262 - shard 60\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00061-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 62000/68262 - shard 61\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00062-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 63000/68262 - shard 62\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00063-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 64000/68262 - shard 63\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00064-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 65000/68262 - shard 64\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00065-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 66000/68262 - shard 65\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00066-of-00067.tfrecord\n",
      "Shard Size 1020\n",
      "\n",
      "Converting image 67000/68262 - shard 66\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_train_00067-of-00067.tfrecord\n",
      "Shard Size 1009\n",
      "\n",
      "Converting image 68000/68262 - shard 67\n",
      "\n",
      "#######################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_convert_dataset(\n",
    "    split_name             = 'train', \n",
    "    filenames              = training_filenames, \n",
    "    class_names_to_ids     = class_names_to_ids,\n",
    "    dataset_dir            = FLAGS.dataset_dir,\n",
    "    output_dir             = FLAGS.output_dir,\n",
    "    tfrecord_filename      = FLAGS.tfrecord_filename,\n",
    "    _NUM_SHARDS            = FLAGS.num_shards_training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00001-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 1000/45508 - shard 1\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00002-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 2000/45508 - shard 2\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00003-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 3000/45508 - shard 3\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00004-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 4000/45508 - shard 4\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00005-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 5000/45508 - shard 5\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00006-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 6000/45508 - shard 6\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00007-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 7000/45508 - shard 7\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00008-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 8000/45508 - shard 8\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00009-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 9000/45508 - shard 9\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00010-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 10000/45508 - shard 10\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00011-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 11000/45508 - shard 11\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00012-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 12000/45508 - shard 12\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00013-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 13000/45508 - shard 13\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00014-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 14000/45508 - shard 14\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00015-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 15000/45508 - shard 15\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00016-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 16000/45508 - shard 16\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00017-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 17000/45508 - shard 17\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00018-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 18000/45508 - shard 18\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00019-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 19000/45508 - shard 19\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00020-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 20000/45508 - shard 20\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00021-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 21000/45508 - shard 21\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00022-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 22000/45508 - shard 22\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00023-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 23000/45508 - shard 23\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00024-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 24000/45508 - shard 24\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00025-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 25000/45508 - shard 25\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00026-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 26000/45508 - shard 26\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00027-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 27000/45508 - shard 27\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00028-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 28000/45508 - shard 28\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00029-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 29000/45508 - shard 29\n",
      "Converting image 30000/45508 - shard 29\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00030-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 31000/45508 - shard 30\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00031-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 32000/45508 - shard 31\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00032-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 33000/45508 - shard 32\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00033-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 34000/45508 - shard 33\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00034-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 35000/45508 - shard 34\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00035-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 36000/45508 - shard 35\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00036-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 37000/45508 - shard 36\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00037-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 38000/45508 - shard 37\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00038-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 39000/45508 - shard 38\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00039-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 40000/45508 - shard 39\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00040-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 41000/45508 - shard 40\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00041-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 42000/45508 - shard 41\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00042-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 43000/45508 - shard 42\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00043-of-00044.tfrecord\n",
      "Shard Size 1036\n",
      "\n",
      "Converting image 44000/45508 - shard 43\n",
      "\n",
      "#######################\n",
      "\n",
      "Processing TFRecord File: data_prepared/dmunet_augmented_dataset_validation_00044-of-00044.tfrecord\n",
      "Shard Size 1004\n",
      "\n",
      "Converting image 45000/45508 - shard 44\n",
      "\n",
      "#######################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_convert_dataset(\n",
    "    split_name             = 'validation', \n",
    "    filenames              = validation_filenames, \n",
    "    class_names_to_ids     = class_names_to_ids,\n",
    "    dataset_dir            = FLAGS.dataset_dir,\n",
    "    output_dir             = FLAGS.output_dir,\n",
    "    tfrecord_filename      = FLAGS.tfrecord_filename,\n",
    "    _NUM_SHARDS            = FLAGS.num_shards_validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Finally, we write a labels file that will be useful as a reference later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_to_class_names = dict(zip(range(len(class_names)), class_names))\n",
    "write_label_file(labels_to_class_names, FLAGS.output_dir, filename=\"labels.txt\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
