{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DICE - Notebook 3.1 - Model Training and Transfer Learning - Cleaned Data\n",
    "\n",
    "<br/>\n",
    "\n",
    "```\n",
    "*************************************************************************\n",
    "**\n",
    "** 2017 Mai 23\n",
    "**\n",
    "** In place of a legal notice, here is a blessing:\n",
    "**\n",
    "**    May you do good and not evil.\n",
    "**    May you find forgiveness for yourself and forgive others.\n",
    "**    May you share freely, never taking more than you give.\n",
    "**\n",
    "*************************************************************************\n",
    "```\n",
    "\n",
    "<table style=\"width:100%; font-size:14px; margin: 20px 0;\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Contact: </b><a href=\"mailto:contact@jonathandekhtiar.eu\" target=\"_blank\">contact@jonathandekhtiar.eu</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Twitter: </b><a href=\"https://twitter.com/born2data\" target=\"_blank\">@born2data</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Tech. Blog: </b><a href=\"http://www.born2data.com/\" target=\"_blank\">born2data.com</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Personal Website: </b><a href=\"http://www.jonathandekhtiar.eu\" target=\"_blank\">jonathandekhtiar.eu</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>RSS Feed: </b><a href=\"https://www.feedcrunch.io/@dataradar/\" target=\"_blank\">FeedCrunch.io</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>LinkedIn: </b><a href=\"https://fr.linkedin.com/in/jonathandekhtiar\" target=\"_blank\">JonathanDEKHTIAR</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook aims to perform the actual transfer learning from the [ImageNet](http://www.image-net.org/) dataset to our custom dataset. For this we will load the model previously trained and retrain the last layers in order to obtain predictions on new classes.\n",
    "\n",
    "A wide variety of models has been trained and made available by the Google Team: https://github.com/tensorflow/models/tree/master/slim\n",
    "\n",
    "We will use in this Notebook, one of the most famous Deep Learning Model: GoogLeNet (aka. Inception-V1) developed by Christian Szegedy and published on ArXiv: https://arxiv.org/abs/1409.4842\n",
    "\n",
    "This notebook will use [Tensorflow-Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) to ease the understanding and reduce the code complexity.\n",
    "\n",
    "Download Inception-V1 Model: http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\n",
    "\n",
    "---\n",
    "\n",
    "As reminder before starting, the data have already been preprocessed (resized, augmented, etc.) in the first Notebook: **[DICE - Notebook 1 - Dataset Augmentation](https://github.com/DEKHTIARJonathan/DICE-DMU_Imagery_Classification_Engine/blob/master/DICE%20-%20Notebook%201%20-%20Dataset%20Augmentation.ipynb)**\n",
    "\n",
    "The preprocessed data all have been saved as **JPEG images** and thus we will only focus on these data.\n",
    "\n",
    "## 1. Notebook Initialisation\n",
    "\n",
    "### 1.1. Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, time, math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2 Initialise global variables and application Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "#State your dataset directory\n",
    "flags.DEFINE_string('dataset_dir', 'data_prepared', 'String: Your dataset directory')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('output_dir', 'output/cleaned', 'String: The output directory where model-checkpoints will be saved')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('inception_dir', 'inception_files', 'String: The output directory where model-checkpoints will be saved')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('labels_dir', 'data_prepared', 'String: The output directory where model-checkpoints will be saved')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('tf_record_start_name', 'dmunet_cleaned_dataset_', 'String: The output filename to name your TFRecord file')\n",
    "\n",
    "#State the number of epochs to train\n",
    "flags.DEFINE_integer('training_epochs', 10, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#State your batch size => Choose the highest value which doesn't give you a memory error.\n",
    "flags.DEFINE_integer('batch_size', 100, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#Learning rate information and configuration (Up to you to experiment)\n",
    "flags.DEFINE_float('initial_learning_rate', 1e-4, 'Float: The proportion of examples in the dataset to be used for validation')\n",
    "\n",
    "flags.DEFINE_float('learning_rate_decay_factor', 0.8, 'Float: The proportion of examples')\n",
    "\n",
    "flags.DEFINE_integer('num_epochs_before_decay', 1, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "# Choose between \"tf.train.SaverDef.V2\" and \"tf.train.SaverDef.V1\". The V1 version is deprecated since Tensorflow r1.0.0\n",
    "flags.DEFINE_integer('tf_saver', tf.train.SaverDef.V1, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#Set the verbosity to INFO level => highest to lowest logging level: DEBUG > INFO > WARN > ERROR > FATAL  \n",
    "flags.DEFINE_integer('tf_logging_level', tf.logging.INFO, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('checkpoint_basename', 'dmunet_cleaned_data.ckpt', 'String: The output filename to name your TFRecord file')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3 Complementary imports from the inception directory set by the flags above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(FLAGS.inception_dir)\n",
    "\n",
    "from preprocessing      import inception_preprocessing\n",
    "from nets.inception_v1  import inception_v1, inception_v1_arg_scope\n",
    "from datasets           import dataset_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Check and Model Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================ Placeholders Definition ================\n",
    "\n",
    "training_step = tf.placeholder(tf.bool)\n",
    "\n",
    "# ================ Additional Derived Variable ================\n",
    "\n",
    "checkpoint_dir  = os.path.join(FLAGS.inception_dir, \"models\")\n",
    "checkpoint_file = os.path.join(checkpoint_dir, \"inception_v1.ckpt\")\n",
    "labels_file     = os.path.join(FLAGS.labels_dir, \"labels.txt\")\n",
    "\n",
    "image_size      = inception_v1.default_image_size # 224 (width and height in pixels)\n",
    "\n",
    "#Create the file pattern of your TFRecord files so that it could be recognized later on\n",
    "file_pattern    = FLAGS.tf_record_start_name + '%s_*.tfrecord'\n",
    "\n",
    "tf.logging.set_verbosity(FLAGS.tf_logging_level) \n",
    "\n",
    "#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\n",
    "\n",
    "items_to_descriptions = {\n",
    "    'image': 'A 3-channel RGB coloured flower image that is either tulips, sunflowers, roses, dandelion, or daisy.',\n",
    "    'label': 'A label that is as such -- 0:daisy, 1:dandelion, 2:roses, 3:sunflowers, 4:tulips'\n",
    "}\n",
    "\n",
    "# =================== Environment Checking ====================\n",
    "\n",
    "#Create the log directory here. Must be done here otherwise import will activate this unneededly.\n",
    "if not os.path.exists(FLAGS.output_dir):\n",
    "    os.mkdir(FLAGS.output_dir)\n",
    "    \n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "if not os.path.isfile(checkpoint_file):\n",
    "    # We download first the TARGZ archive, if necessary, and then extract it.\n",
    "    \n",
    "    targz = \"inception_v1_2016_08_28.tar.gz\"\n",
    "    url = \"http://download.tensorflow.org/models/\" + targz\n",
    "    \n",
    "    tarfilepath = os.path.join(checkpoint_dir, targz)\n",
    "    \n",
    "    if os.path.isfile(tarfilepath):\n",
    "        import tarfile\n",
    "        tarfile.open(tarfilepath, 'r:gz').extractall(checkpoint_dir)\n",
    "    else:\n",
    "        dataset_utils.download_and_uncompress_tarball(url, checkpoint_dir)\n",
    "        \n",
    "    # Get rid of tarfile source (the checkpoint itself will remain)\n",
    "    os.unlink(tarfilepath)\n",
    "\n",
    "\n",
    "if not os.path.isfile(labels_file):\n",
    "    raise Exception(\"The Label File does not exists\")\n",
    "else:\n",
    "    #State the labels file and read it   \n",
    "    labels = open(labels_file, 'r')\n",
    "    \n",
    "    #Create a dictionary to refer each label to their string name\n",
    "    \n",
    "    labels_to_name = dict()\n",
    "    \n",
    "    for line in labels:\n",
    "        label, string_name = line.split(':')\n",
    "        string_name = string_name[:-1] #Remove newline\n",
    "        labels_to_name[int(label)] = string_name\n",
    "\n",
    "    #State the number of classes to predict\n",
    "    num_classes = len(labels_to_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and Validation Datasets Loading\n",
    "\n",
    "### 3.1. Create a function that loads TFRecords Files and return a Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============== DATASET LOADING ======================\n",
    "# We now create a function that creates a Dataset class which will give us many TFRecord files \n",
    "#to feed in the examples into a queue in parallel.\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=file_pattern):\n",
    "    '''\n",
    "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
    "    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "    Your file_pattern is very important in locating the files later. \n",
    "\n",
    "    INPUTS:\n",
    "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "    '''\n",
    "\n",
    "    #First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation']: \n",
    "        err = 'The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name)\n",
    "        raise ValueError(err)\n",
    "    \n",
    "    file_pattern_for_counting = file_pattern % (split_name)\n",
    "    \n",
    "    #Count the total number of examples in all of these shard    \n",
    "    tfrecords_to_count = [\n",
    "        os.path.join(dataset_dir, file) \n",
    "        for file in os.listdir(dataset_dir) \n",
    "        if file.startswith(file_pattern_for_counting[:-10]) # We remove the 10 last chars: *.tfrecord   \n",
    "    ]\n",
    "    \n",
    "    num_samples = 0\n",
    "    \n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "            num_samples += 1\n",
    "\n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "      'image/class/label': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image(),\n",
    "        'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "    #Create the labels_to_name file\n",
    "    labels_to_name_dict = labels_to_name\n",
    "    \n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = os.path.join(dataset_dir, file_pattern_for_counting)\n",
    "\n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = file_pattern_path,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        num_readers = 4,\n",
    "        num_samples = num_samples,\n",
    "        num_classes = num_classes,\n",
    "        labels_to_name = labels_to_name_dict,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Create a function that return a batch of data for training or validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=True):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "\n",
    "    '''\n",
    "    #First create the data_provider object\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        common_queue_capacity = 24 + 3 * batch_size,\n",
    "        common_queue_min = 24)\n",
    "\n",
    "    #Obtain the raw image using the get method\n",
    "    raw_image, label = data_provider.get(['image', 'label'])\n",
    "\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\n",
    "\n",
    "    #As for the raw images, we just do a simple reshape to batch it up\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    images, raw_images, labels = tf.train.batch(\n",
    "        [image, raw_image, label],\n",
    "        batch_size = batch_size,\n",
    "        num_threads = 4,\n",
    "        capacity = 4 * batch_size,\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return images, raw_images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Loading training and validation datasets and defining associated data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_dataset = get_split('train', FLAGS.dataset_dir, file_pattern=file_pattern)\n",
    "training_batch = load_batch(training_dataset, batch_size=FLAGS.batch_size)\n",
    "\n",
    "validation_dataset = get_split('validation', FLAGS.dataset_dir, file_pattern=file_pattern)\n",
    "validation_batch = load_batch(training_dataset, batch_size=FLAGS.batch_size, is_training=False)\n",
    "\n",
    "images, _, labels = tf.cond(\n",
    "                        training_step,\n",
    "                        lambda: training_batch,\n",
    "                        lambda: validation_batch\n",
    "                    )\n",
    "\n",
    "num_training_steps_per_epoch = math.ceil(training_dataset.num_samples / FLAGS.batch_size)\n",
    "num_validation_steps_per_epoch = math.ceil(validation_dataset.num_samples / FLAGS.batch_size)\n",
    "\n",
    "#Know the number steps to take before decaying the learning rate and batches per epoch\n",
    "training_decay_steps = FLAGS.num_epochs_before_decay * num_training_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the model inference\n",
    "with slim.arg_scope(inception_v1_arg_scope()):\n",
    "    logits, end_points = inception_v1(images, num_classes = training_dataset.num_classes, is_training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the scopes that you want to exclude for restoration\n",
    "exclude              = [\"InceptionV1/Logits\", \"InceptionV1/AuxLogits\"]\n",
    "variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n",
    "variables_to_save    = slim.get_variables_to_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)\n",
    "one_hot_labels = slim.one_hot_encoding(labels, training_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n",
    "total_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the global step for monitoring the learning_rate and training.\n",
    "global_step = get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define your exponentially decaying learning rate\n",
    "lr = tf.train.exponential_decay(\n",
    "    learning_rate = FLAGS.initial_learning_rate,\n",
    "    global_step = global_step,\n",
    "    decay_steps = training_decay_steps,\n",
    "    decay_rate = FLAGS.learning_rate_decay_factor,\n",
    "    staircase = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we can define the optimizer that takes on the learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the train_op.\n",
    "train_op = slim.learning.create_train_op(total_loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
    "with tf.name_scope('metrics'):\n",
    "    probabilities              = end_points['Predictions']\n",
    "    predictions                = tf.argmax(probabilities, 1)\n",
    "\n",
    "    accuracy, accuracy_update  = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "    top5_acc, top5_acc_update  = tf.contrib.metrics.streaming_sparse_recall_at_k(probabilities, labels, 5)\n",
    "\n",
    "    metrics_op                 = tf.group(accuracy_update, top5_acc_update)\n",
    "    \n",
    "stream_vars = [i for i in tf.local_variables() if i.name.split('/')[0] == 'metrics']\n",
    "reset_op = [tf.variables_initializer(stream_vars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "\n",
    "with tf.name_scope('training_summaries'):\n",
    "\n",
    "    train_loss    = tf.summary.scalar('total_Loss', total_loss)\n",
    "    train_t1_acc  = tf.summary.scalar('top1-accuracy', accuracy)\n",
    "    train_t5_acc  = tf.summary.scalar('top5-accuracy', top5_acc)\n",
    "    train_lr      = tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "    training_summary_op =   tf.summary.merge(\n",
    "        [train_loss, train_t1_acc, train_t5_acc, train_lr], \n",
    "        name=\"train__summaries\"\n",
    "    )\n",
    "    \n",
    "with tf.name_scope('validation_summaries'):\n",
    "\n",
    "    validation_loss    = tf.summary.scalar('total_Loss', total_loss)\n",
    "    validation_t1_acc  = tf.summary.scalar('top1-accuracy', accuracy)\n",
    "    validation_t5_acc  = tf.summary.scalar('top5-accuracy', top5_acc)\n",
    "    validation_lr      = tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "    validation_summary_op =   tf.summary.merge(\n",
    "        [validation_loss, validation_t1_acc, validation_t5_acc, validation_lr], \n",
    "        name=\"validation__summaries\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n",
    "def train_step(sess, train_op, global_step):\n",
    "    '''\n",
    "    Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n",
    "    '''\n",
    "    #Check the time for each sess run\n",
    "    start_time = time.time()\n",
    "    total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op], feed_dict={training_step: True})\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "    #Run the logging to print some results\n",
    "    logging.info('Training Step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n",
    "\n",
    "    return total_loss, global_step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a evaluation step function\n",
    "def perform_validation(sess):\n",
    "    '''\n",
    "    Simply read all the validation batches and run a classification run\n",
    "    '''\n",
    "    \n",
    "    sess.run(reset_op) # We reset the streaming Metrics for validation check\n",
    "\n",
    "    for val_step in range(num_validation_steps_per_epoch):\n",
    "        if (val_step % 100  == 0 and val_step != 0):\n",
    "            logging.info('Validation: Running Val Step: %s', val_step)\n",
    "        _ = sess.run(metrics_op, feed_dict={training_step: False})\n",
    "\n",
    "    accuracy_value, top5_acc_value = sess.run([accuracy, top5_acc])\n",
    "\n",
    "    logging.info('Validation: Top-1 Accuracy: %s', accuracy_value)\n",
    "    logging.info('Validation: Top-5 Accuracy: %s', top5_acc_value)\n",
    "\n",
    "    sv.summary_computed(\n",
    "        sess, \n",
    "        sess.run(validation_summary_op, feed_dict={training_step: False})\n",
    "    )\n",
    "\n",
    "    sess.run(reset_op) # We reset the streaming Metrics for the next training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we create a saver function that actually restores the variables from a checkpoint file in a sess\n",
    "restore_saver = tf.train.Saver(\n",
    "    var_list      = variables_to_restore,\n",
    "    write_version = FLAGS.tf_saver\n",
    ")\n",
    "\n",
    "def restore_fn(sess):\n",
    "    return restore_saver.restore(sess, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define your supervisor for running a managed session. \n",
    "#Do not run the summary_op automatically or else it will consume too much memory\n",
    "\n",
    "saving_saver = tf.train.Saver(\n",
    "    var_list      = variables_to_save,\n",
    "    write_version = FLAGS.tf_saver, \n",
    "    max_to_keep   = FLAGS.training_epochs\n",
    ")\n",
    "\n",
    "sv = tf.train.Supervisor(\n",
    "    logdir                = FLAGS.output_dir,\n",
    "    summary_op            = None, \n",
    "    init_fn               = restore_fn,\n",
    "    checkpoint_basename   = FLAGS.checkpoint_basename,\n",
    "    save_model_secs       = None, # Prevent Automatic Model saving\n",
    "    saver                 = saving_saver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from inception_files\\models\\inception_v1.ckpt\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:\n",
      "###################################\n",
      "\n",
      "INFO:tensorflow:Number of Training Epochs: 10\n",
      "INFO:tensorflow:Number of Training Steps per Epoch: 23\n",
      "INFO:tensorflow:Total Number of Training Steps: 230\n",
      "\n",
      "INFO:tensorflow:Number of Validation Runs: 4\n",
      "INFO:tensorflow:Number of Validation Steps per Validation Run: 15\n",
      "INFO:tensorflow:Total Number of Validation Steps: 60\n",
      "\n",
      "INFO:tensorflow:Total Number of Steps: 290\n",
      "\n",
      "INFO:tensorflow:Summary Recorded Every 2 Training Steps\n",
      "INFO:tensorflow:\n",
      "###################################\n",
      "\n",
      "INFO:tensorflow:Epoch 1/10\n",
      "INFO:tensorflow:Training: Learning Rate: 0.0001\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.0\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: nan\n",
      "INFO:tensorflow:Validation: Top-1 Accuracy: 0.209333\n",
      "INFO:tensorflow:Validation: Top-5 Accuracy: 0.612\n",
      "INFO:tensorflow:Training Step 1: loss: 2.0698 (7.64 sec/step)\n",
      "INFO:tensorflow:Training Step 2: loss: 1.8348 (1.15 sec/step)\n",
      "INFO:tensorflow:Training Step 3: loss: 1.7260 (1.14 sec/step)\n",
      "INFO:tensorflow:Training Step 4: loss: 1.6594 (1.14 sec/step)\n",
      "INFO:tensorflow:Training Step 5: loss: 1.4745 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 6: loss: 1.5465 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 7: loss: 1.3499 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 8: loss: 1.3085 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 9: loss: 1.1824 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 10: loss: 1.0732 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 11: loss: 1.0720 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 12: loss: 1.0037 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 13: loss: 0.9170 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 14: loss: 0.8802 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 15: loss: 0.9105 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 16: loss: 0.8135 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 17: loss: 0.8826 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 18: loss: 0.8819 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 19: loss: 0.9107 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 20: loss: 0.7125 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 21: loss: 0.8359 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 22: loss: 0.7335 (1.33 sec/step)\n",
      "INFO:tensorflow:Training Step 23: loss: 0.7529 (1.30 sec/step)\n",
      "INFO:tensorflow:Epoch 2/10\n",
      "INFO:tensorflow:Training: Learning Rate: 8e-05\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.636957\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: 0.893043478261\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Training Step 24: loss: 0.7335 (1.11 sec/step)\n",
      "INFO:tensorflow:Training Step 25: loss: 0.7177 (1.13 sec/step)\n",
      "INFO:tensorflow:Training Step 26: loss: 0.6827 (1.11 sec/step)\n",
      "INFO:tensorflow:Training Step 27: loss: 0.8139 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 28: loss: 0.7784 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 29: loss: 0.5988 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 30: loss: 0.7500 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 31: loss: 0.5752 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 32: loss: 0.6262 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 33: loss: 0.6283 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 34: loss: 0.6798 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 35: loss: 0.6713 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 36: loss: 0.6428 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 37: loss: 0.7933 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 38: loss: 0.5413 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 39: loss: 0.5712 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 40: loss: 0.6350 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 41: loss: 0.7035 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 42: loss: 0.6353 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 43: loss: 0.7257 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 44: loss: 0.6339 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 45: loss: 0.4416 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 46: loss: 0.6626 (1.30 sec/step)\n",
      "INFO:tensorflow:Epoch 3/10\n",
      "INFO:tensorflow:Training: Learning Rate: 6.4e-05\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.735435\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: 0.93847826087\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:global_step/sec: 0.383242\n",
      "INFO:tensorflow:Training Step 47: loss: 0.5323 (1.12 sec/step)\n",
      "INFO:tensorflow:Training Step 48: loss: 0.5979 (1.11 sec/step)\n",
      "INFO:tensorflow:Training Step 49: loss: 0.5642 (1.12 sec/step)\n",
      "INFO:tensorflow:Training Step 50: loss: 0.5292 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 51: loss: 0.5564 (1.33 sec/step)\n",
      "INFO:tensorflow:Training Step 52: loss: 0.6030 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 53: loss: 0.5837 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 54: loss: 0.5810 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 55: loss: 0.5793 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 56: loss: 0.5139 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 57: loss: 0.5157 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 58: loss: 0.5561 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 59: loss: 0.4656 (1.33 sec/step)\n",
      "INFO:tensorflow:Training Step 60: loss: 0.6274 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 61: loss: 0.5310 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 62: loss: 0.5117 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 63: loss: 0.6518 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 64: loss: 0.4584 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 65: loss: 0.5517 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 66: loss: 0.5500 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 67: loss: 0.6510 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 68: loss: 0.4668 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 69: loss: 0.5536 (1.32 sec/step)\n",
      "INFO:tensorflow:Epoch 4/10\n",
      "INFO:tensorflow:Training: Learning Rate: 5.12e-05\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.778696\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: 0.953913043478\n",
      "INFO:tensorflow:Validation: Top-1 Accuracy: 0.927333\n",
      "INFO:tensorflow:Validation: Top-5 Accuracy: 0.994666666667\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Training Step 70: loss: 0.4794 (1.13 sec/step)\n",
      "INFO:tensorflow:Training Step 71: loss: 0.5511 (1.11 sec/step)\n",
      "INFO:tensorflow:Training Step 72: loss: 0.5768 (1.11 sec/step)\n",
      "INFO:tensorflow:Training Step 73: loss: 0.3900 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 74: loss: 0.4345 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 75: loss: 0.4858 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 76: loss: 0.4287 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 77: loss: 0.4749 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 78: loss: 0.4535 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 79: loss: 0.4997 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 80: loss: 0.5098 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 81: loss: 0.5102 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 82: loss: 0.4905 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 83: loss: 0.4998 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 84: loss: 0.4840 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 85: loss: 0.5082 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 86: loss: 0.4257 (1.30 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Training Step 87: loss: 0.4549 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 88: loss: 0.6569 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 89: loss: 0.5408 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 90: loss: 0.4505 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 91: loss: 0.5154 (1.33 sec/step)\n",
      "INFO:tensorflow:Training Step 92: loss: 0.5112 (1.35 sec/step)\n",
      "INFO:tensorflow:Epoch 5/10\n",
      "INFO:tensorflow:Training: Learning Rate: 4.096e-05\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.898261\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: 0.989130434783\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Training Step 93: loss: 0.4557 (1.12 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.391746\n",
      "INFO:tensorflow:Training Step 94: loss: 0.5428 (1.12 sec/step)\n",
      "INFO:tensorflow:Training Step 95: loss: 0.5152 (1.13 sec/step)\n",
      "INFO:tensorflow:Training Step 96: loss: 0.5989 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 97: loss: 0.4893 (1.38 sec/step)\n",
      "INFO:tensorflow:Training Step 98: loss: 0.5566 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 99: loss: 0.4529 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 100: loss: 0.4384 (1.33 sec/step)\n",
      "INFO:tensorflow:Training Step 101: loss: 0.4767 (1.36 sec/step)\n",
      "INFO:tensorflow:Training Step 102: loss: 0.4384 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 103: loss: 0.4429 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 104: loss: 0.4132 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 105: loss: 0.3772 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 106: loss: 0.4483 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 107: loss: 0.4155 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 108: loss: 0.3723 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 109: loss: 0.4684 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 110: loss: 0.4188 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 111: loss: 0.4935 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 112: loss: 0.3980 (1.34 sec/step)\n",
      "INFO:tensorflow:Training Step 113: loss: 0.4703 (1.37 sec/step)\n",
      "INFO:tensorflow:Training Step 114: loss: 0.4122 (1.36 sec/step)\n",
      "INFO:tensorflow:Training Step 115: loss: 0.4319 (1.36 sec/step)\n",
      "INFO:tensorflow:Epoch 6/10\n",
      "INFO:tensorflow:Training: Learning Rate: 3.2768e-05\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.901739\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: 0.988913043478\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Training Step 116: loss: 0.5128 (1.12 sec/step)\n",
      "INFO:tensorflow:Training Step 117: loss: 0.5087 (1.13 sec/step)\n",
      "INFO:tensorflow:Training Step 118: loss: 0.4400 (1.11 sec/step)\n",
      "INFO:tensorflow:Training Step 119: loss: 0.4970 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 120: loss: 0.4716 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 121: loss: 0.5308 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 122: loss: 0.5104 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 123: loss: 0.4135 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 124: loss: 0.4554 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 125: loss: 0.5433 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 126: loss: 0.3905 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 127: loss: 0.4737 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 128: loss: 0.4534 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 129: loss: 0.3938 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 130: loss: 0.3403 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 131: loss: 0.4900 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 132: loss: 0.4840 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 133: loss: 0.4867 (1.33 sec/step)\n",
      "INFO:tensorflow:Training Step 134: loss: 0.4797 (1.35 sec/step)\n",
      "INFO:tensorflow:Training Step 135: loss: 0.5828 (1.35 sec/step)\n",
      "INFO:tensorflow:Training Step 136: loss: 0.3766 (1.36 sec/step)\n",
      "INFO:tensorflow:Training Step 137: loss: 0.3870 (1.37 sec/step)\n",
      "INFO:tensorflow:Training Step 138: loss: 0.5174 (1.35 sec/step)\n",
      "INFO:tensorflow:Epoch 7/10\n",
      "INFO:tensorflow:Training: Learning Rate: 2.62144e-05\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.89942\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: 0.988985507246\n",
      "INFO:tensorflow:Validation: Top-1 Accuracy: 0.946\n",
      "INFO:tensorflow:Validation: Top-5 Accuracy: 0.998\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Training Step 139: loss: 0.4445 (1.12 sec/step)\n",
      "INFO:tensorflow:Training Step 140: loss: 0.4036 (1.12 sec/step)\n",
      "INFO:tensorflow:Training Step 141: loss: 0.3856 (1.12 sec/step)\n",
      "INFO:tensorflow:Training Step 142: loss: 0.4374 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 143: loss: 0.4545 (1.30 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.416597\n",
      "INFO:tensorflow:Training Step 144: loss: 0.4623 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 145: loss: 0.4071 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 146: loss: 0.4598 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 147: loss: 0.3949 (1.38 sec/step)\n",
      "INFO:tensorflow:Training Step 148: loss: 0.4504 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 149: loss: 0.4866 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 150: loss: 0.4589 (1.38 sec/step)\n",
      "INFO:tensorflow:Training Step 151: loss: 0.3987 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 152: loss: 0.4878 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 153: loss: 0.3437 (1.34 sec/step)\n",
      "INFO:tensorflow:Training Step 154: loss: 0.4052 (1.35 sec/step)\n",
      "INFO:tensorflow:Training Step 155: loss: 0.4222 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 156: loss: 0.5949 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 157: loss: 0.3669 (1.35 sec/step)\n",
      "INFO:tensorflow:Training Step 158: loss: 0.3878 (1.37 sec/step)\n",
      "INFO:tensorflow:Training Step 159: loss: 0.4343 (1.37 sec/step)\n",
      "INFO:tensorflow:Training Step 160: loss: 0.4026 (1.37 sec/step)\n",
      "INFO:tensorflow:Training Step 161: loss: 0.4662 (1.36 sec/step)\n",
      "INFO:tensorflow:Epoch 8/10\n",
      "INFO:tensorflow:Training: Learning Rate: 2.09715e-05\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.914783\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: 0.988695652174\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Training Step 162: loss: 0.4137 (1.14 sec/step)\n",
      "INFO:tensorflow:Training Step 163: loss: 0.5086 (1.11 sec/step)\n",
      "INFO:tensorflow:Training Step 164: loss: 0.4277 (1.11 sec/step)\n",
      "INFO:tensorflow:Training Step 165: loss: 0.4538 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 166: loss: 0.4621 (1.29 sec/step)\n",
      "INFO:tensorflow:Training Step 167: loss: 0.4106 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 168: loss: 0.4687 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 169: loss: 0.4173 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 170: loss: 0.5153 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 171: loss: 0.4975 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 172: loss: 0.4079 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 173: loss: 0.4765 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 174: loss: 0.4614 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 175: loss: 0.4225 (1.31 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Training Step 176: loss: 0.4557 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 177: loss: 0.4299 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 178: loss: 0.4167 (1.34 sec/step)\n",
      "INFO:tensorflow:Training Step 179: loss: 0.3929 (1.41 sec/step)\n",
      "INFO:tensorflow:Training Step 180: loss: 0.5284 (1.36 sec/step)\n",
      "INFO:tensorflow:Training Step 181: loss: 0.4781 (1.39 sec/step)\n",
      "INFO:tensorflow:Training Step 182: loss: 0.3193 (1.36 sec/step)\n",
      "INFO:tensorflow:Training Step 183: loss: 0.4344 (1.37 sec/step)\n",
      "INFO:tensorflow:Training Step 184: loss: 0.3927 (1.36 sec/step)\n",
      "INFO:tensorflow:Epoch 9/10\n",
      "INFO:tensorflow:Training: Learning Rate: 1.67772e-05\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.911739\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: 0.989347826087\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Training Step 185: loss: 0.3903 (1.12 sec/step)\n",
      "INFO:tensorflow:Training Step 186: loss: 0.4172 (1.10 sec/step)\n",
      "INFO:tensorflow:Training Step 187: loss: 0.4001 (1.11 sec/step)\n",
      "INFO:tensorflow:Training Step 188: loss: 0.3601 (1.34 sec/step)\n",
      "INFO:tensorflow:Training Step 189: loss: 0.4933 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 190: loss: 0.3819 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 191: loss: 0.4384 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 192: loss: 0.4316 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 193: loss: 0.4254 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 194: loss: 0.5462 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 195: loss: 0.4000 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 196: loss: 0.3124 (1.34 sec/step)\n",
      "INFO:tensorflow:Training Step 197: loss: 0.4375 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 198: loss: 0.3502 (1.31 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.458374\n",
      "INFO:tensorflow:Training Step 199: loss: 0.4794 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 200: loss: 0.5199 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 201: loss: 0.3369 (1.35 sec/step)\n",
      "INFO:tensorflow:Training Step 202: loss: 0.4287 (1.39 sec/step)\n",
      "INFO:tensorflow:Training Step 203: loss: 0.5034 (1.39 sec/step)\n",
      "INFO:tensorflow:Training Step 204: loss: 0.3801 (1.41 sec/step)\n",
      "INFO:tensorflow:Training Step 205: loss: 0.5586 (1.37 sec/step)\n",
      "INFO:tensorflow:Training Step 206: loss: 0.3657 (1.39 sec/step)\n",
      "INFO:tensorflow:Training Step 207: loss: 0.4582 (1.36 sec/step)\n",
      "INFO:tensorflow:Epoch 10/10\n",
      "INFO:tensorflow:Training: Learning Rate: 1.34218e-05\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.911594\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: 0.990434782609\n",
      "INFO:tensorflow:Validation: Top-1 Accuracy: 0.965333\n",
      "INFO:tensorflow:Validation: Top-5 Accuracy: 0.997333333333\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Training Step 208: loss: 0.4267 (1.12 sec/step)\n",
      "INFO:tensorflow:Training Step 209: loss: 0.3239 (1.09 sec/step)\n",
      "INFO:tensorflow:Training Step 210: loss: 0.4031 (1.09 sec/step)\n",
      "INFO:tensorflow:Training Step 211: loss: 0.3701 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 212: loss: 0.4904 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 213: loss: 0.4118 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 214: loss: 0.3852 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 215: loss: 0.5735 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 216: loss: 0.4496 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 217: loss: 0.3897 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 218: loss: 0.4011 (1.31 sec/step)\n",
      "INFO:tensorflow:Training Step 219: loss: 0.5598 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 220: loss: 0.4894 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 221: loss: 0.4443 (1.32 sec/step)\n",
      "INFO:tensorflow:Training Step 222: loss: 0.4705 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 223: loss: 0.3880 (1.30 sec/step)\n",
      "INFO:tensorflow:Training Step 224: loss: 0.3267 (1.34 sec/step)\n",
      "INFO:tensorflow:Training Step 225: loss: 0.3633 (1.36 sec/step)\n",
      "INFO:tensorflow:Training Step 226: loss: 0.4415 (1.36 sec/step)\n",
      "INFO:tensorflow:Training Step 227: loss: 0.3777 (1.41 sec/step)\n",
      "INFO:tensorflow:Training Step 228: loss: 0.3856 (1.39 sec/step)\n",
      "INFO:tensorflow:Training Step 229: loss: 0.3410 (1.38 sec/step)\n",
      "INFO:tensorflow:Training Step 230: loss: 0.3463 (1.36 sec/step)\n",
      "INFO:tensorflow:\n",
      "#############################################\n",
      "INFO:tensorflow:############## Final Statistics ##############\n",
      "INFO:tensorflow:#############################################\n",
      "\n",
      "INFO:tensorflow:Training: Loss: 0.346275\n",
      "INFO:tensorflow:Training: Learning Rate: 1.34218e-05\n",
      "INFO:tensorflow:Training: Top-1 Accuracy: 0.917826\n",
      "INFO:tensorflow:Training: Top-5 Accuracy: 0.993913043478\n",
      "INFO:tensorflow:Validation: Top-1 Accuracy: 0.955333\n",
      "INFO:tensorflow:Validation: Top-5 Accuracy: 0.997333333333\n",
      "INFO:tensorflow:Finished training! Saving model to disk now.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n"
     ]
    }
   ],
   "source": [
    "#Run the managed session\n",
    "with sv.managed_session() as sess:   \n",
    "    \n",
    "    logging.info(\"\\n###################################\\n\")\n",
    "    \n",
    "    total_training_steps   = num_training_steps_per_epoch * FLAGS.training_epochs\n",
    "    \n",
    "    total_validation_runs  = FLAGS.training_epochs // 3 + 1\n",
    "    total_validation_steps = num_validation_steps_per_epoch * total_validation_runs\n",
    "    \n",
    "    total_steps            = total_training_steps + total_validation_steps\n",
    "    \n",
    "    logging.info(\"Number of Training Epochs: %d\", FLAGS.training_epochs)\n",
    "    logging.info(\"Number of Training Steps per Epoch: %d\", num_training_steps_per_epoch)  \n",
    "    logging.info(\"Total Number of Training Steps: %d\\n\", total_training_steps)  \n",
    "    \n",
    "    logging.info(\"Number of Validation Runs: %d\", total_validation_runs)\n",
    "    logging.info(\"Number of Validation Steps per Validation Run: %d\", num_validation_steps_per_epoch)\n",
    "    logging.info(\"Total Number of Validation Steps: %d\\n\", total_validation_steps)  \n",
    "    \n",
    "    logging.info(\"Total Number of Steps: %d\\n\", total_steps) \n",
    "    \n",
    "    logging.info(\"Summary Recorded Every %d Training Steps\" % round(num_training_steps_per_epoch/10))\n",
    "    \n",
    "    logging.info(\"\\n###################################\\n\")\n",
    "    \n",
    "    for step in range(num_training_steps_per_epoch * FLAGS.training_epochs):\n",
    "            \n",
    "        #At the start of every epoch, show the vital information:\n",
    "        if step % num_training_steps_per_epoch == 0:\n",
    "            \n",
    "            learning_rate_value, accuracy_value, top5_acc_value = sess.run([lr, accuracy, top5_acc])\n",
    "            \n",
    "            logging.info('Epoch %d/%d', step/num_training_steps_per_epoch + 1, FLAGS.training_epochs)\n",
    "            \n",
    "            logging.info('Training: Learning Rate: %s', learning_rate_value)\n",
    "            logging.info('Training: Top-1 Accuracy: %s', accuracy_value)\n",
    "            logging.info('Training: Top-5 Accuracy: %s', top5_acc_value)\n",
    "            \n",
    "            # Compute Validation Metrics after 3 Epochs \n",
    "            \n",
    "            if step % (num_training_steps_per_epoch * 3) == 0: \n",
    "                perform_validation(sess)\n",
    "            \n",
    "            # Save Model after each Epoch\n",
    "            if step != 0:\n",
    "                sv.saver.save(sess, sv.save_path, global_step = sv.global_step)                    \n",
    "\n",
    "        #Log the summaries every 1-10th of epoch.\n",
    "        if (step % num_training_steps_per_epoch % round(num_training_steps_per_epoch/10)) == 0 :\n",
    "            loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "            sv.summary_computed(\n",
    "                sess, \n",
    "                sess.run(training_summary_op, feed_dict={training_step: True})\n",
    "            )\n",
    "\n",
    "        #If not, simply run the training step\n",
    "        else:\n",
    "            loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "\n",
    "    #We log the final training and validation statistics\n",
    "    \n",
    "    logging.info('\\n#############################################')\n",
    "    logging.info('############## Final Statistics ##############')\n",
    "    logging.info('#############################################\\n')\n",
    "    \n",
    "    logging.info('Training: Loss: %s', loss)\n",
    "    logging.info('Training: Learning Rate: %s', learning_rate_value)\n",
    "    logging.info('Training: Top-1 Accuracy: %s', sess.run(accuracy))\n",
    "    logging.info('Training: Top-5 Accuracy: %s', sess.run(top5_acc))\n",
    "    \n",
    "    perform_validation(sess) \n",
    "\n",
    "    #Once all the training has been done, save the log files and checkpoint model\n",
    "    logging.info('Finished training! Saving model to disk now.')\n",
    "    \n",
    "    sv.saver.save(sess, sv.save_path, global_step = sv.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
