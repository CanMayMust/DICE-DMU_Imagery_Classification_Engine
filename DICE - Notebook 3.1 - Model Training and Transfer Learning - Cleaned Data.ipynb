{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICE - Notebook 3.1 - Model Training and Transfer Learning - Cleaned Data\n",
    "\n",
    "<br/>\n",
    "\n",
    "```\n",
    "*************************************************************************\n",
    "**\n",
    "** 2017 Mai 23\n",
    "**\n",
    "** In place of a legal notice, here is a blessing:\n",
    "**\n",
    "**    May you do good and not evil.\n",
    "**    May you find forgiveness for yourself and forgive others.\n",
    "**    May you share freely, never taking more than you give.\n",
    "**\n",
    "*************************************************************************\n",
    "```\n",
    "\n",
    "<table style=\"width:100%; font-size:14px; margin: 20px 0;\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Contact: </b><a href=\"mailto:contact@jonathandekhtiar.eu\" target=\"_blank\">contact@jonathandekhtiar.eu</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Twitter: </b><a href=\"https://twitter.com/born2data\" target=\"_blank\">@born2data</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Tech. Blog: </b><a href=\"http://www.born2data.com/\" target=\"_blank\">born2data.com</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>Personal Website: </b><a href=\"http://www.jonathandekhtiar.eu\" target=\"_blank\">jonathandekhtiar.eu</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>RSS Feed: </b><a href=\"https://www.feedcrunch.io/@dataradar/\" target=\"_blank\">FeedCrunch.io</a>\n",
    "        </td>\n",
    "        <td style=\"text-align:center\">\n",
    "            <b>LinkedIn: </b><a href=\"https://fr.linkedin.com/in/jonathandekhtiar\" target=\"_blank\">JonathanDEKHTIAR</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook aims to perform the actual transfer learning from the [ImageNet](http://www.image-net.org/) dataset to our custom dataset. For this we will load the model previously trained and retrain the last layers in order to obtain predictions on new classes.\n",
    "\n",
    "A wide variety of models has been trained and made available by the Google Team: https://github.com/tensorflow/models/tree/master/slim\n",
    "\n",
    "We will use in this Notebook, one of the most famous Deep Learning Model: GoogLeNet (aka. Inception-V1) developed by Christian Szegedy and published on ArXiv: https://arxiv.org/abs/1409.4842\n",
    "\n",
    "This notebook will use [Tensorflow-Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) to ease the understanding and reduce the code complexity.\n",
    "\n",
    "Download Inception-V1 Model: http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\n",
    "\n",
    "---\n",
    "\n",
    "As reminder before starting, the data have already been preprocessed (resized, augmented, etc.) in the first Notebook: **[DICE - Notebook 1 - Dataset Augmentation](https://github.com/DEKHTIARJonathan/DICE-DMU_Imagery_Classification_Engine/blob/master/DICE%20-%20Notebook%201%20-%20Dataset%20Augmentation.ipynb)**\n",
    "\n",
    "The preprocessed data all have been saved as **JPEG images** and thus we will only focus on these data.\n",
    "\n",
    "## 1. Notebook Initialisation\n",
    "\n",
    "### 1.1. Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, time, math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2 Initialise global variables and application Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "#State your dataset directory\n",
    "flags.DEFINE_string('dataset_dir', 'data_prepared', 'String: Your dataset directory')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('output_dir', 'output/cleaned', 'String: The output directory where model-checkpoints will be saved')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('inception_dir', 'inception_files', 'String: The output directory where model-checkpoints will be saved')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('labels_dir', 'data_prepared', 'String: The output directory where model-checkpoints will be saved')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('tf_record_start_name', 'dmunet_cleaned_dataset_', 'String: The output filename to name your TFRecord file')\n",
    "\n",
    "#State the number of epochs to train\n",
    "flags.DEFINE_integer('training_epochs', 10, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#State your batch size => Choose the highest value which doesn't give you a memory error.\n",
    "flags.DEFINE_integer('batch_size', 110, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#Learning rate information and configuration (Up to you to experiment)\n",
    "flags.DEFINE_float('initial_learning_rate', 1e-4, 'Float: The proportion of examples in the dataset to be used for validation')\n",
    "\n",
    "flags.DEFINE_float('learning_rate_decay_factor', 0.8, 'Float: The proportion of examples')\n",
    "\n",
    "flags.DEFINE_integer('num_epochs_before_decay', 1, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "# Choose between \"tf.train.SaverDef.V2\" and \"tf.train.SaverDef.V1\". The V1 version is deprecated since Tensorflow r1.0.0\n",
    "flags.DEFINE_integer('tf_saver', tf.train.SaverDef.V1, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#Set the verbosity to INFO level => highest to lowest logging level: DEBUG > INFO > WARN > ERROR > FATAL  \n",
    "flags.DEFINE_integer('tf_logging_level', tf.logging.INFO, 'Int: Number of shards to split the TFRecord files into')\n",
    "\n",
    "#Output filename for the naming the TFRecord file\n",
    "flags.DEFINE_string('checkpoint_basename', 'dmunet_cleaned_data.ckpt', 'String: The output filename to name your TFRecord file')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3 Complementary imports from the inception directory set by the flags above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(FLAGS.inception_dir)\n",
    "\n",
    "from preprocessing      import inception_preprocessing\n",
    "from nets.inception_v1  import inception_v1, inception_v1_arg_scope\n",
    "from datasets           import dataset_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Check and Model Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ================ Additional Derived Variable ================\n",
    "\n",
    "checkpoint_dir  = os.path.join(FLAGS.inception_dir, \"models\")\n",
    "checkpoint_file = os.path.join(checkpoint_dir, \"inception_v1.ckpt\")\n",
    "labels_file     = os.path.join(FLAGS.labels_dir, \"labels.txt\")\n",
    "\n",
    "image_size      = inception_v1.default_image_size # 224 (width and height in pixels)\n",
    "\n",
    "#Create the file pattern of your TFRecord files so that it could be recognized later on\n",
    "file_pattern    = FLAGS.tf_record_start_name + '%s_*.tfrecord'\n",
    "\n",
    "tf.logging.set_verbosity(FLAGS.tf_logging_level) \n",
    "\n",
    "#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\n",
    "\n",
    "items_to_descriptions = {\n",
    "    'image': 'A 3-channel RGB coloured flower image that is either tulips, sunflowers, roses, dandelion, or daisy.',\n",
    "    'label': 'A label that is as such -- 0:daisy, 1:dandelion, 2:roses, 3:sunflowers, 4:tulips'\n",
    "}\n",
    "\n",
    "# =================== Environment Checking ====================\n",
    "\n",
    "#Create the log directory here. Must be done here otherwise import will activate this unneededly.\n",
    "if not os.path.exists(FLAGS.output_dir):\n",
    "    os.mkdir(FLAGS.output_dir)\n",
    "    \n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "if not os.path.isfile(checkpoint_file):\n",
    "    # We download first the TARGZ archive, if necessary, and then extract it.\n",
    "    \n",
    "    targz = \"inception_v1_2016_08_28.tar.gz\"\n",
    "    url = \"http://download.tensorflow.org/models/\" + targz\n",
    "    \n",
    "    tarfilepath = os.path.join(checkpoint_dir, targz)\n",
    "    \n",
    "    if os.path.isfile(tarfilepath):\n",
    "        import tarfile\n",
    "        tarfile.open(tarfilepath, 'r:gz').extractall(checkpoint_dir)\n",
    "    else:\n",
    "        dataset_utils.download_and_uncompress_tarball(url, checkpoint_dir)\n",
    "        \n",
    "    # Get rid of tarfile source (the checkpoint itself will remain)\n",
    "    os.unlink(tarfilepath)\n",
    "\n",
    "\n",
    "if not os.path.isfile(labels_file):\n",
    "    raise Exception(\"The Label File does not exists\")\n",
    "else:\n",
    "    #State the labels file and read it   \n",
    "    labels = open(labels_file, 'r')\n",
    "    \n",
    "    #Create a dictionary to refer each label to their string name\n",
    "    \n",
    "    labels_to_name = dict()\n",
    "    \n",
    "    for line in labels:\n",
    "        label, string_name = line.split(':')\n",
    "        string_name = string_name[:-1] #Remove newline\n",
    "        labels_to_name[int(label)] = string_name\n",
    "\n",
    "    #State the number of classes to predict\n",
    "    num_classes = len(labels_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#============== DATASET LOADING ======================\n",
    "# We now create a function that creates a Dataset class which will give us many TFRecord files \n",
    "#to feed in the examples into a queue in parallel.\n",
    "\n",
    "def get_split(split_name, dataset_dir, file_pattern=file_pattern):\n",
    "    '''\n",
    "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
    "    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
    "    Your file_pattern is very important in locating the files later. \n",
    "\n",
    "    INPUTS:\n",
    "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
    "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
    "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
    "\n",
    "    OUTPUTS:\n",
    "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
    "    '''\n",
    "\n",
    "    #First check whether the split_name is train or validation\n",
    "    if split_name not in ['train', 'validation']: \n",
    "        err = 'The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name)\n",
    "        raise ValueError(err)\n",
    "    \n",
    "    file_pattern_for_counting = file_pattern % (split_name)\n",
    "    \n",
    "    #Count the total number of examples in all of these shard    \n",
    "    tfrecords_to_count = [\n",
    "        os.path.join(dataset_dir, file) \n",
    "        for file in os.listdir(dataset_dir) \n",
    "        if file.startswith(file_pattern_for_counting[:-10]) # We remove the 10 last chars: *.tfrecord   \n",
    "    ]\n",
    "    \n",
    "    num_samples = 0\n",
    "    \n",
    "    for tfrecord_file in tfrecords_to_count:\n",
    "        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
    "            num_samples += 1\n",
    "\n",
    "    #Create a reader, which must be a TFRecord reader in this case\n",
    "    reader = tf.TFRecordReader\n",
    "\n",
    "    #Create the keys_to_features dictionary for the decoder\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
    "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
    "      'image/class/label': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "\n",
    "    #Create the items_to_handlers dictionary for the decoder.\n",
    "    items_to_handlers = {\n",
    "        'image': slim.tfexample_decoder.Image(),\n",
    "        'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
    "    }\n",
    "\n",
    "    #Start to create the decoder\n",
    "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
    "\n",
    "    #Create the labels_to_name file\n",
    "    labels_to_name_dict = labels_to_name\n",
    "    \n",
    "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
    "    file_pattern_path = os.path.join(dataset_dir, file_pattern_for_counting)\n",
    "\n",
    "    #Actually create the dataset\n",
    "    dataset = slim.dataset.Dataset(\n",
    "        data_sources = file_pattern_path,\n",
    "        decoder = decoder,\n",
    "        reader = reader,\n",
    "        num_readers = 4,\n",
    "        num_samples = num_samples,\n",
    "        num_classes = num_classes,\n",
    "        labels_to_name = labels_to_name_dict,\n",
    "        items_to_descriptions = items_to_descriptions)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=True):\n",
    "    '''\n",
    "    Loads a batch for training.\n",
    "\n",
    "    INPUTS:\n",
    "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
    "    - batch_size(int): determines how big of a batch to train\n",
    "    - height(int): the height of the image to resize to during preprocessing\n",
    "    - width(int): the width of the image to resize to during preprocessing\n",
    "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
    "\n",
    "    OUTPUTS:\n",
    "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
    "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
    "\n",
    "    '''\n",
    "    #First create the data_provider object\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset,\n",
    "        common_queue_capacity = 24 + 3 * batch_size,\n",
    "        common_queue_min = 24)\n",
    "\n",
    "    #Obtain the raw image using the get method\n",
    "    raw_image, label = data_provider.get(['image', 'label'])\n",
    "\n",
    "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
    "    image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\n",
    "\n",
    "    #As for the raw images, we just do a simple reshape to batch it up\n",
    "    raw_image = tf.expand_dims(raw_image, 0)\n",
    "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
    "    raw_image = tf.squeeze(raw_image)\n",
    "\n",
    "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
    "    images, raw_images, labels = tf.train.batch(\n",
    "        [image, raw_image, label],\n",
    "        batch_size = batch_size,\n",
    "        num_threads = 4,\n",
    "        capacity = 4 * batch_size,\n",
    "        allow_smaller_final_batch = True)\n",
    "\n",
    "    return images, raw_images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset and data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = get_split('train', FLAGS.dataset_dir, file_pattern=file_pattern)\n",
    "images, _, labels = load_batch(dataset, batch_size=FLAGS.batch_size)\n",
    "\n",
    "num_steps_per_epoch = math.ceil(dataset.num_samples / FLAGS.batch_size)\n",
    "\n",
    "#Know the number steps to take before decaying the learning rate and batches per epoch\n",
    "decay_steps = FLAGS.num_epochs_before_decay * num_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the model inference\n",
    "with slim.arg_scope(inception_v1_arg_scope()):\n",
    "    logits, end_points = inception_v1(images, num_classes = dataset.num_classes, is_training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the scopes that you want to exclude for restoration\n",
    "exclude              = [\"InceptionV1/Logits\", \"InceptionV1/AuxLogits\"]\n",
    "variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n",
    "variables_to_save    = slim.get_variables_to_restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)\n",
    "one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks\n",
    "loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n",
    "total_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the global step for monitoring the learning_rate and training.\n",
    "global_step = get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define your exponentially decaying learning rate\n",
    "lr = tf.train.exponential_decay(\n",
    "    learning_rate = FLAGS.initial_learning_rate,\n",
    "    global_step = global_step,\n",
    "    decay_steps = decay_steps,\n",
    "    decay_rate = FLAGS.learning_rate_decay_factor,\n",
    "    staircase = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we can define the optimizer that takes on the learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the train_op.\n",
    "train_op = slim.learning.create_train_op(total_loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
    "predictions                = tf.argmax(end_points['Predictions'], 1)\n",
    "probabilities              = end_points['Predictions']\n",
    "accuracy, accuracy_update  = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
    "metrics_op                 = tf.group(accuracy_update, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now finally create all the summaries you need to monitor and group them into one summary op.\n",
    "tf.summary.scalar('losses/Total_Loss', total_loss)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "tf.summary.scalar('learning_rate', lr)\n",
    "\n",
    "my_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n",
    "def train_step(sess, train_op, global_step):\n",
    "    '''\n",
    "    Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n",
    "    '''\n",
    "    #Check the time for each sess run\n",
    "    start_time = time.time()\n",
    "    total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "    #Run the logging to print some results\n",
    "    logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n",
    "\n",
    "    return total_loss, global_step_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we create a saver function that actually restores the variables from a checkpoint file in a sess\n",
    "restore_saver = tf.train.Saver(\n",
    "    var_list      = variables_to_restore,\n",
    "    write_version = FLAGS.tf_saver\n",
    ")\n",
    "\n",
    "def restore_fn(sess):\n",
    "    return restore_saver.restore(sess, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define your supervisor for running a managed session. \n",
    "#Do not run the summary_op automatically or else it will consume too much memory\n",
    "\n",
    "saving_saver = tf.train.Saver(\n",
    "    var_list      = variables_to_save,\n",
    "    write_version = FLAGS.tf_saver, \n",
    "    max_to_keep   = FLAGS.training_epochs\n",
    ")\n",
    "\n",
    "sv = tf.train.Supervisor(\n",
    "    logdir                = FLAGS.output_dir, \n",
    "    summary_op            = None, \n",
    "    init_fn               = restore_fn,\n",
    "    checkpoint_basename   = FLAGS.checkpoint_basename,\n",
    "    save_model_secs       = None, # Prevent Automatic Model saving\n",
    "    saver                 = saving_saver\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from inception_files\\models\\inception_v1.ckpt\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "\n",
      "###################################\n",
      "\n",
      "Number of Epochs: 10\n",
      "Number of Steps per Epoch: 21\n",
      "Summary Recorded Every 2 Steps\n",
      "\n",
      "total steps: 210\n",
      "\n",
      "###################################\n",
      "\n",
      "INFO:tensorflow:Epoch 1/10\n",
      "INFO:tensorflow:Current Learning Rate: 0.0001\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.0\n",
      "INFO:tensorflow:global step 1: loss: 2.0077 (9.40 sec/step)\n",
      "INFO:tensorflow:global step 2: loss: 1.7862 (1.17 sec/step)\n",
      "INFO:tensorflow:global step 3: loss: 1.7694 (1.17 sec/step)\n",
      "INFO:tensorflow:global step 4: loss: 1.5915 (1.22 sec/step)\n",
      "INFO:tensorflow:global step 5: loss: 1.5314 (1.32 sec/step)\n",
      "INFO:tensorflow:global step 6: loss: 1.4967 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 7: loss: 1.4356 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 8: loss: 1.2424 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 9: loss: 1.1610 (1.32 sec/step)\n",
      "INFO:tensorflow:global step 10: loss: 1.1516 (1.32 sec/step)\n",
      "INFO:tensorflow:global step 11: loss: 1.0799 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 12: loss: 1.0296 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 13: loss: 1.0719 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 14: loss: 0.8724 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 15: loss: 0.8958 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 16: loss: 0.9153 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 17: loss: 0.9158 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 18: loss: 0.8018 (1.38 sec/step)\n",
      "INFO:tensorflow:global step 19: loss: 0.7098 (1.32 sec/step)\n",
      "INFO:tensorflow:global step 20: loss: 0.7709 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 21: loss: 0.8606 (1.33 sec/step)\n",
      "INFO:tensorflow:Epoch 2/10\n",
      "INFO:tensorflow:Current Learning Rate: 8e-05\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.624242\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:global step 22: loss: 0.8290 (1.19 sec/step)\n",
      "INFO:tensorflow:global step 23: loss: 0.8988 (1.17 sec/step)\n",
      "INFO:tensorflow:global step 24: loss: 0.7513 (1.18 sec/step)\n",
      "INFO:tensorflow:global step 25: loss: 0.7297 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 26: loss: 0.7457 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 27: loss: 0.7014 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 28: loss: 0.7137 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 29: loss: 0.7736 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 30: loss: 0.6918 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 31: loss: 0.7816 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 32: loss: 0.6054 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 33: loss: 0.6660 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 34: loss: 0.6665 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 35: loss: 0.5090 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 36: loss: 0.5846 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 37: loss: 0.6603 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 38: loss: 0.4848 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 39: loss: 0.6112 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 40: loss: 0.6333 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 41: loss: 0.6601 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 42: loss: 0.5960 (1.36 sec/step)\n",
      "INFO:tensorflow:Epoch 3/10\n",
      "INFO:tensorflow:Current Learning Rate: 6.4e-05\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.728571\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:global step 43: loss: 0.6073 (1.20 sec/step)\n",
      "INFO:tensorflow:global step 44: loss: 0.6353 (1.16 sec/step)\n",
      "INFO:tensorflow:global step 45: loss: 0.7310 (1.17 sec/step)\n",
      "INFO:tensorflow:global step 46: loss: 0.5912 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 47: loss: 0.4917 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 48: loss: 0.5071 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 49: loss: 0.5507 (1.33 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.408334\n",
      "INFO:tensorflow:global step 50: loss: 0.5974 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 51: loss: 0.5722 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 52: loss: 0.4816 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 53: loss: 0.5838 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 54: loss: 0.5212 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 55: loss: 0.4197 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 56: loss: 0.4981 (1.36 sec/step)\n",
      "INFO:tensorflow:global step 57: loss: 0.4484 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 58: loss: 0.5819 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 59: loss: 0.5572 (1.44 sec/step)\n",
      "INFO:tensorflow:global step 60: loss: 0.6142 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 61: loss: 0.6291 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 62: loss: 0.6282 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 63: loss: 0.5500 (1.34 sec/step)\n",
      "INFO:tensorflow:Epoch 4/10\n",
      "INFO:tensorflow:Current Learning Rate: 5.12e-05\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.776768\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:global step 64: loss: 0.4891 (1.20 sec/step)\n",
      "INFO:tensorflow:global step 65: loss: 0.5335 (1.17 sec/step)\n",
      "INFO:tensorflow:global step 66: loss: 0.4480 (1.17 sec/step)\n",
      "INFO:tensorflow:global step 67: loss: 0.5011 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 68: loss: 0.5777 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 69: loss: 0.4875 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 70: loss: 0.5289 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 71: loss: 0.4371 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 72: loss: 0.5006 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 73: loss: 0.4295 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 74: loss: 0.5852 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 75: loss: 0.4812 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 76: loss: 0.5517 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 77: loss: 0.5299 (1.40 sec/step)\n",
      "INFO:tensorflow:global step 78: loss: 0.5023 (1.37 sec/step)\n",
      "INFO:tensorflow:global step 79: loss: 0.5739 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 80: loss: 0.4076 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 81: loss: 0.4649 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 82: loss: 0.5669 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 83: loss: 0.4575 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 84: loss: 0.5581 (1.35 sec/step)\n",
      "INFO:tensorflow:Epoch 5/10\n",
      "INFO:tensorflow:Current Learning Rate: 4.096e-05\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.805087\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:global step 85: loss: 0.4992 (1.19 sec/step)\n",
      "INFO:tensorflow:global step 86: loss: 0.5793 (1.16 sec/step)\n",
      "INFO:tensorflow:global step 87: loss: 0.4256 (1.17 sec/step)\n",
      "INFO:tensorflow:global step 88: loss: 0.4280 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 89: loss: 0.4031 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 90: loss: 0.5304 (1.37 sec/step)\n",
      "INFO:tensorflow:global step 91: loss: 0.5123 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 92: loss: 0.4407 (1.34 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global step 93: loss: 0.4240 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 94: loss: 0.5413 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 95: loss: 0.4477 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 96: loss: 0.5068 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 97: loss: 0.5643 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 98: loss: 0.4516 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 99: loss: 0.5287 (1.32 sec/step)\n",
      "INFO:tensorflow:global step 100: loss: 0.4665 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 101: loss: 0.4780 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 102: loss: 0.4091 (1.35 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.441664\n",
      "INFO:tensorflow:global step 103: loss: 0.4414 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 104: loss: 0.4392 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 105: loss: 0.5664 (1.40 sec/step)\n",
      "INFO:tensorflow:Epoch 6/10\n",
      "INFO:tensorflow:Current Learning Rate: 3.2768e-05\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.823983\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:global step 106: loss: 0.5265 (1.19 sec/step)\n",
      "INFO:tensorflow:global step 107: loss: 0.4903 (1.16 sec/step)\n",
      "INFO:tensorflow:global step 108: loss: 0.4573 (1.16 sec/step)\n",
      "INFO:tensorflow:global step 109: loss: 0.4360 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 110: loss: 0.5301 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 111: loss: 0.3791 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 112: loss: 0.5221 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 113: loss: 0.4135 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 114: loss: 0.4209 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 115: loss: 0.4598 (1.36 sec/step)\n",
      "INFO:tensorflow:global step 116: loss: 0.5315 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 117: loss: 0.4743 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 118: loss: 0.3987 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 119: loss: 0.4085 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 120: loss: 0.4718 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 121: loss: 0.5160 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 122: loss: 0.5590 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 123: loss: 0.4841 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 124: loss: 0.5160 (1.40 sec/step)\n",
      "INFO:tensorflow:global step 125: loss: 0.4113 (1.41 sec/step)\n",
      "INFO:tensorflow:global step 126: loss: 0.4824 (1.40 sec/step)\n",
      "INFO:tensorflow:Epoch 7/10\n",
      "INFO:tensorflow:Current Learning Rate: 2.62144e-05\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.836003\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:global step 127: loss: 0.3501 (1.19 sec/step)\n",
      "INFO:tensorflow:global step 128: loss: 0.3638 (1.16 sec/step)\n",
      "INFO:tensorflow:global step 129: loss: 0.3208 (1.16 sec/step)\n",
      "INFO:tensorflow:global step 130: loss: 0.4383 (1.32 sec/step)\n",
      "INFO:tensorflow:global step 131: loss: 0.4743 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 132: loss: 0.5507 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 133: loss: 0.4564 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 134: loss: 0.5073 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 135: loss: 0.4962 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 136: loss: 0.4315 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 137: loss: 0.3961 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 138: loss: 0.4097 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 139: loss: 0.4342 (1.38 sec/step)\n",
      "INFO:tensorflow:global step 140: loss: 0.4024 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 141: loss: 0.4091 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 142: loss: 0.4421 (1.43 sec/step)\n",
      "INFO:tensorflow:global step 143: loss: 0.4171 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 144: loss: 0.4074 (1.41 sec/step)\n",
      "INFO:tensorflow:global step 145: loss: 0.4203 (1.41 sec/step)\n",
      "INFO:tensorflow:global step 146: loss: 0.3876 (1.41 sec/step)\n",
      "INFO:tensorflow:global step 147: loss: 0.4190 (1.41 sec/step)\n",
      "INFO:tensorflow:Epoch 8/10\n",
      "INFO:tensorflow:Current Learning Rate: 2.09715e-05\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.847743\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:global_step/sec: 0.374971\n",
      "INFO:tensorflow:global step 148: loss: 0.4294 (1.20 sec/step)\n",
      "INFO:tensorflow:global step 149: loss: 0.4879 (1.16 sec/step)\n",
      "INFO:tensorflow:global step 150: loss: 0.3870 (1.16 sec/step)\n",
      "INFO:tensorflow:global step 151: loss: 0.4804 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 152: loss: 0.4189 (1.32 sec/step)\n",
      "INFO:tensorflow:global step 153: loss: 0.4320 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 154: loss: 0.3952 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 155: loss: 0.4933 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 156: loss: 0.5012 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 157: loss: 0.4400 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 158: loss: 0.4867 (1.36 sec/step)\n",
      "INFO:tensorflow:global step 159: loss: 0.3873 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 160: loss: 0.3986 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 161: loss: 0.4486 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 162: loss: 0.4361 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 163: loss: 0.3806 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 164: loss: 0.4400 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 165: loss: 0.4226 (1.46 sec/step)\n",
      "INFO:tensorflow:global step 166: loss: 0.4445 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 167: loss: 0.4849 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 168: loss: 0.4593 (1.44 sec/step)\n",
      "INFO:tensorflow:Epoch 9/10\n",
      "INFO:tensorflow:Current Learning Rate: 1.67772e-05\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.855898\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:global step 169: loss: 0.4717 (1.19 sec/step)\n",
      "INFO:tensorflow:global step 170: loss: 0.5815 (1.16 sec/step)\n",
      "INFO:tensorflow:global step 171: loss: 0.3731 (1.17 sec/step)\n",
      "INFO:tensorflow:global step 172: loss: 0.3661 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 173: loss: 0.3867 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 174: loss: 0.4120 (1.37 sec/step)\n",
      "INFO:tensorflow:global step 175: loss: 0.3581 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 176: loss: 0.4465 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 177: loss: 0.3314 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 178: loss: 0.4907 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 179: loss: 0.4592 (1.37 sec/step)\n",
      "INFO:tensorflow:global step 180: loss: 0.4476 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 181: loss: 0.4135 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 182: loss: 0.4463 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 183: loss: 0.5023 (1.44 sec/step)\n",
      "INFO:tensorflow:global step 184: loss: 0.4221 (1.40 sec/step)\n",
      "INFO:tensorflow:global step 185: loss: 0.3092 (1.44 sec/step)\n",
      "INFO:tensorflow:global step 186: loss: 0.4166 (1.44 sec/step)\n",
      "INFO:tensorflow:global step 187: loss: 0.4109 (1.44 sec/step)\n",
      "INFO:tensorflow:global step 188: loss: 0.3421 (1.44 sec/step)\n",
      "INFO:tensorflow:global step 189: loss: 0.2986 (1.43 sec/step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Epoch 10/10\n",
      "INFO:tensorflow:Current Learning Rate: 1.34218e-05\n",
      "INFO:tensorflow:Current Streaming Accuracy: 0.862771\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:global step 190: loss: 0.3673 (1.20 sec/step)\n",
      "INFO:tensorflow:global step 191: loss: 0.4617 (1.17 sec/step)\n",
      "INFO:tensorflow:global step 192: loss: 0.5034 (1.18 sec/step)\n",
      "INFO:tensorflow:global step 193: loss: 0.4107 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 194: loss: 0.3982 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 195: loss: 0.4339 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 196: loss: 0.3426 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 197: loss: 0.3564 (1.38 sec/step)\n",
      "INFO:tensorflow:global step 198: loss: 0.3858 (1.35 sec/step)\n",
      "INFO:tensorflow:global_step/sec: 0.425035\n",
      "INFO:tensorflow:global step 199: loss: 0.3514 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 200: loss: 0.4797 (1.33 sec/step)\n",
      "INFO:tensorflow:global step 201: loss: 0.3671 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 202: loss: 0.4307 (1.34 sec/step)\n",
      "INFO:tensorflow:global step 203: loss: 0.4511 (1.35 sec/step)\n",
      "INFO:tensorflow:global step 204: loss: 0.4875 (1.39 sec/step)\n",
      "INFO:tensorflow:global step 205: loss: 0.3666 (1.40 sec/step)\n",
      "INFO:tensorflow:global step 206: loss: 0.4055 (1.43 sec/step)\n",
      "INFO:tensorflow:global step 207: loss: 0.4893 (1.41 sec/step)\n",
      "INFO:tensorflow:global step 208: loss: 0.4694 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 209: loss: 0.3487 (1.42 sec/step)\n",
      "INFO:tensorflow:global step 210: loss: 0.4186 (1.45 sec/step)\n",
      "INFO:tensorflow:Final Loss: 0.418595\n",
      "INFO:tensorflow:Final Accuracy: 0.868442\n",
      "INFO:tensorflow:Finished training! Saving model to disk now.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n"
     ]
    }
   ],
   "source": [
    "#Run the managed session\n",
    "with sv.managed_session() as sess:   \n",
    "    \n",
    "    print(\"\\n###################################\\n\")\n",
    "    \n",
    "    print(\"Number of Epochs: %d\" % FLAGS.training_epochs)\n",
    "    print(\"Number of Steps per Epoch: %d\" % num_steps_per_epoch)\n",
    "    print(\"Summary Recorded Every %d Steps\\n\" % round(num_steps_per_epoch/10))\n",
    "    \n",
    "    print(\"total steps: %d\" % (num_steps_per_epoch * FLAGS.training_epochs))\n",
    "    \n",
    "    print(\"\\n###################################\\n\")\n",
    "    \n",
    "    for step in range(num_steps_per_epoch * FLAGS.training_epochs):\n",
    "        \n",
    "        #At the start of every epoch, show the vital information:\n",
    "        if step % num_steps_per_epoch == 0:\n",
    "            \n",
    "            learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n",
    "            \n",
    "            logging.info('Epoch %d/%d', step/num_steps_per_epoch + 1, FLAGS.training_epochs)\n",
    "            logging.info('Current Learning Rate: %s', learning_rate_value)\n",
    "            logging.info('Current Streaming Accuracy: %s', accuracy_value)\n",
    "            \n",
    "            # We save the model after each epoch\n",
    "            if step != 0:\n",
    "                sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n",
    "\n",
    "        #Log the summaries every 1-10th of epoch.\n",
    "        if (step % num_steps_per_epoch % round(num_steps_per_epoch/10)) == 0 :\n",
    "            loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "            summaries = sess.run(my_summary_op)\n",
    "            sv.summary_computed(sess, summaries)\n",
    "\n",
    "        #If not, simply run the training step\n",
    "        else:\n",
    "            loss, _ = train_step(sess, train_op, sv.global_step)\n",
    "\n",
    "    #We log the final training loss and accuracy\n",
    "    logging.info('Final Loss: %s', loss)\n",
    "    logging.info('Final Accuracy: %s', sess.run(accuracy))\n",
    "\n",
    "    #Once all the training has been done, save the log files and checkpoint model\n",
    "    logging.info('Finished training! Saving model to disk now.')\n",
    "    \n",
    "    sv.saver.save(sess, sv.save_path, global_step = sv.global_step)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
